{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is the code check input directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T23:17:04.380190Z",
     "iopub.status.busy": "2024-05-30T23:17:04.379841Z",
     "iopub.status.idle": "2024-05-30T23:17:04.784819Z",
     "shell.execute_reply": "2024-05-30T23:17:04.783772Z",
     "shell.execute_reply.started": "2024-05-30T23:17:04.380161Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T23:17:05.546219Z",
     "iopub.status.busy": "2024-05-30T23:17:05.545905Z",
     "iopub.status.idle": "2024-05-30T23:17:10.895820Z",
     "shell.execute_reply": "2024-05-30T23:17:10.895056Z",
     "shell.execute_reply.started": "2024-05-30T23:17:05.546194Z"
    },
    "papermill": {
     "duration": 6.722973,
     "end_time": "2024-05-13T19:17:41.586735",
     "exception": false,
     "start_time": "2024-05-13T19:17:34.863762",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel,AutoTokenizer\n",
    "import torch, torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T23:17:10.898948Z",
     "iopub.status.busy": "2024-05-30T23:17:10.898415Z",
     "iopub.status.idle": "2024-05-30T23:17:10.905182Z",
     "shell.execute_reply": "2024-05-30T23:17:10.904232Z",
     "shell.execute_reply.started": "2024-05-30T23:17:10.898914Z"
    },
    "papermill": {
     "duration": 0.017389,
     "end_time": "2024-05-13T19:17:41.613232",
     "exception": false,
     "start_time": "2024-05-13T19:17:41.595843",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state.detach().cpu()\n",
    "    input_mask_expanded = (\n",
    "        attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    )\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\n",
    "        input_mask_expanded.sum(1), min=1e-9\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T23:17:10.906731Z",
     "iopub.status.busy": "2024-05-30T23:17:10.906402Z",
     "iopub.status.idle": "2024-05-30T23:17:10.915029Z",
     "shell.execute_reply": "2024-05-30T23:17:10.914162Z",
     "shell.execute_reply.started": "2024-05-30T23:17:10.906702Z"
    },
    "papermill": {
     "duration": 0.017943,
     "end_time": "2024-05-13T19:17:41.6398",
     "exception": false,
     "start_time": "2024-05-13T19:17:41.621857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EmbedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,df,tokenizer,max_length):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max = max_length\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self,idx):\n",
    "        text = self.df.loc[idx,\"full_text\"]\n",
    "        tokens = self.tokenizer(\n",
    "                text,\n",
    "                None,\n",
    "                add_special_tokens=True,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=self.max,\n",
    "                return_tensors=\"pt\")\n",
    "        tokens = {k:v.squeeze(0) for k,v in tokens.items()}\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.008495,
     "end_time": "2024-05-13T19:17:41.656901",
     "exception": false,
     "start_time": "2024-05-13T19:17:41.648406",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Extract Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T23:17:10.916944Z",
     "iopub.status.busy": "2024-05-30T23:17:10.916369Z",
     "iopub.status.idle": "2024-05-30T23:17:10.933653Z",
     "shell.execute_reply": "2024-05-30T23:17:10.932849Z",
     "shell.execute_reply.started": "2024-05-30T23:17:10.916914Z"
    },
    "papermill": {
     "duration": 0.026311,
     "end_time": "2024-05-13T19:17:41.691963",
     "exception": false,
     "start_time": "2024-05-13T19:17:41.665652",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_embeddings(model_name='', max_length=1024, batch_size=32, compute_train=True, compute_test=True):\n",
    "\n",
    "    global train, test\n",
    "\n",
    "    DEVICE = \"cuda:1\" # EXTRACT EMBEDDINGS WITH GPU #2\n",
    "    path = \"/kaggle/input/download-huggingface-models/\"\n",
    "    disk_name = path + model_name.replace(\"/\",\"_\")\n",
    "    model = AutoModel.from_pretrained( disk_name , trust_remote_code=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained( disk_name , trust_remote_code=True)\n",
    "\n",
    "    ds_tr = EmbedDataset(train, tokenizer, max_length)\n",
    "    embed_dataloader_tr = torch.utils.data.DataLoader(ds_tr,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=False)\n",
    "    ds_te = EmbedDataset(test, tokenizer, max_length)\n",
    "    embed_dataloader_te = torch.utils.data.DataLoader(ds_te,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=False)\n",
    "    \n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    # COMPUTE TRAIN EMBEDDINGS\n",
    "    all_train_text_feats = []\n",
    "    if compute_train:\n",
    "        for batch in tqdm(embed_dataloader_tr,total=len(embed_dataloader_tr)):\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                with torch.cuda.amp.autocast(enabled=True):\n",
    "                    model_output = model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "            sentence_embeddings = mean_pooling(model_output, attention_mask.detach().cpu())\n",
    "            # Normalize the embeddings\n",
    "            sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "            sentence_embeddings =  sentence_embeddings.squeeze(0).detach().cpu().numpy()\n",
    "            all_train_text_feats.extend(sentence_embeddings)\n",
    "    all_train_text_feats = np.array(all_train_text_feats)\n",
    "\n",
    "    # COMPUTE TEST EMBEDDINGS\n",
    "    all_test_text_feats = []\n",
    "    if compute_test:\n",
    "        for batch in embed_dataloader_te:\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                with torch.cuda.amp.autocast(enabled=True):\n",
    "                    model_output = model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "            sentence_embeddings = mean_pooling(model_output, attention_mask.detach().cpu())\n",
    "            # Normalize the embeddings\n",
    "            sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "            sentence_embeddings =  sentence_embeddings.squeeze(0).detach().cpu().numpy()\n",
    "            all_test_text_feats.extend(sentence_embeddings)\n",
    "        all_test_text_feats = np.array(all_test_text_feats)\n",
    "    all_test_text_feats = np.array(all_test_text_feats)\n",
    "\n",
    "    # CLEAR MEMORY\n",
    "    del ds_tr, ds_te\n",
    "    del embed_dataloader_tr, embed_dataloader_te\n",
    "    del model, tokenizer\n",
    "    del model_output, sentence_embeddings, input_ids, attention_mask\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # RETURN EMBEDDINGS\n",
    "    return all_train_text_feats, all_test_text_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T23:17:10.935216Z",
     "iopub.status.busy": "2024-05-30T23:17:10.934700Z",
     "iopub.status.idle": "2024-05-30T23:17:10.944794Z",
     "shell.execute_reply": "2024-05-30T23:17:10.944055Z",
     "shell.execute_reply.started": "2024-05-30T23:17:10.935186Z"
    },
    "papermill": {
     "duration": 0.016991,
     "end_time": "2024-05-13T19:17:41.717967",
     "exception": false,
     "start_time": "2024-05-13T19:17:41.700976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# EMBEDDINGS TO LOAD/COMPUTE\n",
    "# PARAMETERS = (MODEL_NAME, MAX_LENGTH, BATCH_SIZE)\n",
    "# CHOOSE LARGEST BATCH SIZE WITHOUT MEMORY ERROR\n",
    "\n",
    "models = [\n",
    "    ('microsoft/deberta-base', 1024, 32),\n",
    "    ('microsoft/deberta-large', 1024, 8),\n",
    "    ('microsoft/deberta-v3-large', 1024, 8),\n",
    "    ('allenai/longformer-base-4096', 1024, 32),\n",
    "    ('google/bigbird-roberta-base', 1024, 32),\n",
    "    ('google/bigbird-roberta-large', 1024, 8),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T23:17:10.945996Z",
     "iopub.status.busy": "2024-05-30T23:17:10.945753Z",
     "iopub.status.idle": "2024-05-30T23:18:04.914854Z",
     "shell.execute_reply": "2024-05-30T23:18:04.913900Z",
     "shell.execute_reply.started": "2024-05-30T23:17:10.945976Z"
    },
    "papermill": {
     "duration": 51.388457,
     "end_time": "2024-05-13T19:18:33.115494",
     "exception": false,
     "start_time": "2024-05-13T19:17:41.727037",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = \"/kaggle/input/essay-embeddings-v1/\"\n",
    "all_train_embeds = []\n",
    "all_test_embeds = []\n",
    "\n",
    "for (model, max_length, batch_size) in models:\n",
    "    name = path + model.replace(\"/\",\"_\") + \".npy\"\n",
    "    if os.path.exists(name):\n",
    "        _, test_embed = get_embeddings(model_name=model, max_length=max_length, batch_size=batch_size, compute_train=False)\n",
    "        train_embed = np.load(name)\n",
    "        print(f\"Loading train embeddings for {name}\")\n",
    "    else:\n",
    "        print(f\"Computing train embeddings for {name}\")\n",
    "        train_embed, test_embed = get_embeddings(model_name=model, max_length=max_length, batch_size=batch_size, compute_train=True)\n",
    "        np.save(name, train_embed)\n",
    "    all_train_embeds.append(train_embed)\n",
    "    all_test_embeds.append(test_embed)\n",
    "\n",
    "del train_embed, test_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T23:18:04.916898Z",
     "iopub.status.busy": "2024-05-30T23:18:04.916151Z",
     "iopub.status.idle": "2024-05-30T23:18:04.923150Z",
     "shell.execute_reply": "2024-05-30T23:18:04.922222Z",
     "shell.execute_reply.started": "2024-05-30T23:18:04.916863Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Number of models:', len(all_train_embeds))\n",
    "print('Length of dataset:', len(all_train_embeds[0]))\n",
    "for i in range(len(models)):\n",
    "    print(f'Embedding size of model {i}: {len(all_train_embeds[0][i])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE ENGINEERING of text\n",
    "In this part, we create some new features without fine-tuning bag of words, tf-idf, or sentence embeddings directly on the training data.\n",
    "\n",
    "The original feature engineering link: https://www.kaggle.com/code/deepaksingh47/feature-engineeing-lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T23:18:04.928565Z",
     "iopub.status.busy": "2024-05-30T23:18:04.927992Z",
     "iopub.status.idle": "2024-05-30T23:18:37.859488Z",
     "shell.execute_reply": "2024-05-30T23:18:37.858341Z",
     "shell.execute_reply.started": "2024-05-30T23:18:04.928533Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install \"/kaggle/input/pyspellchecker/pyspellchecker-0.7.2-py3-none-any.whl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T23:18:37.861406Z",
     "iopub.status.busy": "2024-05-30T23:18:37.861098Z",
     "iopub.status.idle": "2024-05-30T23:18:51.987084Z",
     "shell.execute_reply": "2024-05-30T23:18:51.986386Z",
     "shell.execute_reply.started": "2024-05-30T23:18:37.861374Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import polars as pl\n",
    "import warnings\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import json, string\n",
    "import transformers\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\n",
    "from transformers import DataCollatorWithPadding\n",
    "from datasets import Dataset,load_dataset, load_from_disk\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_metric, disable_progress_bar\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "from sklearn.model_selection import KFold, GroupKFold, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from collections import Counter\n",
    "import spacy\n",
    "import re\n",
    "from spellchecker import SpellChecker\n",
    "import lightgbm as lgb\n",
    "\n",
    "# logging setting \n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "logging.disable(logging.ERROR)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "disable_progress_bar()\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T23:18:51.988893Z",
     "iopub.status.busy": "2024-05-30T23:18:51.988628Z",
     "iopub.status.idle": "2024-05-30T23:18:52.400218Z",
     "shell.execute_reply": "2024-05-30T23:18:52.399518Z",
     "shell.execute_reply.started": "2024-05-30T23:18:51.988862Z"
    }
   },
   "outputs": [],
   "source": [
    "class PATHS:\n",
    "    train_path = '/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv'\n",
    "    test_path = '/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv'\n",
    "    sub_path = '/kaggle/input/learning-agency-lab-automated-essay-scoring-2/sample_submission.csv'\n",
    "class CFG:\n",
    "    n_splits = 5\n",
    "    seed = 42\n",
    "    num_labels = 6\n",
    "    \n",
    "train = pd.read_csv(PATHS.train_path)\n",
    "test = pd.read_csv(PATHS.test_path)\n",
    "\n",
    "def removeHTML(x):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',x)\n",
    "\n",
    "\n",
    "cList = {\n",
    "    \"ain't\": \"am not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\", \"couldn't've\": \"could not have\", \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n",
    "    \"he'd\": \"he would\",  ## --> he had or he would\n",
    "    \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\", \"he's\": \"he is\", \n",
    "    \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\"how's\": \"how is\",\n",
    "    \"I'd\": \"I would\",   ## --> I had or I would\n",
    "    \"I'd've\": \"I would have\",\"I'll\": \"I will\",\"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\",\"isn't\": \"is not\",\n",
    "    \"it'd\": \"it had\",   ## --> It had or It would\n",
    "    \"it'd've\": \"it would have\",\"it'll\": \"it will\",\"it'll've\": \"it will have\",\"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\"mustn't\": \"must not\",\"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\",   ## --> It had or It would\n",
    "    \"she'd've\": \"she would have\",\"she'll\": \"she will\",\"she'll've\": \"she will have\",\"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\"shouldn't\": \"should not\",\"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\"so's\": \"so is\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that'd've\": \"that would have\",\"that's\": \"that is\",\n",
    "    \"there'd\": \"there had\",\n",
    "    \"there'd've\": \"there would have\",\"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'd've\": \"they would have\",\"they'll\": \"they will\",\"they'll've\": \"they will have\",\"they're\": \"they are\",\"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\"wasn't\": \"was not\",\"weren't\": \"were not\",\n",
    "    \"we'd\": \"we had\",\n",
    "    \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\"we're\": \"we are\",\"we've\": \"we have\",\n",
    "    \"what'll\": \"what will\",\"what'll've\": \"what will have\",\"what're\": \"what are\",\"what's\": \"what is\",\"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",\"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\"where's\": \"where is\",\"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who's\": \"who is\",\"who've\": \"who have\",\"why's\": \"why is\",\"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\"won't\": \"will not\",\"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\"wouldn't\": \"would not\",\"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\"y'alls\": \"you alls\",\"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\"you'd\": \"you had\",\"you'd've\": \"you would have\",\"you'll\": \"you you will\",\"you'll've\": \"you you will have\",\n",
    "    \"you're\": \"you are\",  \"you've\": \"you have\"\n",
    "}\n",
    "c_re = re.compile('(%s)' % '|'.join(cList.keys()))\n",
    "\n",
    "def expandContractions(text):\n",
    "    def replace(match):\n",
    "        return cList[match.group(0)]\n",
    "    return c_re.sub(replace, text)\n",
    "\n",
    "def dataPreprocessing(x):\n",
    "    # Convert words to lowercase\n",
    "    x = x.lower()\n",
    "    # Remove HTML\n",
    "    x = removeHTML(x)\n",
    "    # Delete strings starting with @\n",
    "    x = re.sub(\"@\\w+\", '',x)\n",
    "    # Delete Numbers\n",
    "    x = re.sub(\"'\\d+\", '',x)\n",
    "    x = re.sub(\"\\d+\", '',x)\n",
    "    # Delete URL\n",
    "    x = re.sub(\"http\\w+\", '',x)\n",
    "    # Remove \\xa0\n",
    "    x = x.replace(u'\\xa0',' ')\n",
    "    # Replace consecutive empty spaces with a single space character\n",
    "    x = re.sub(r\"\\s+\", \" \", x)\n",
    "    x = expandContractions(x)\n",
    "    # Replace consecutive commas and periods with one comma and period character\n",
    "    x = re.sub(r\"\\.+\", \".\", x)\n",
    "    x = re.sub(r\"\\,+\", \",\", x)\n",
    "#     x = re.sub(r'[^\\w\\s.,;:\"\"''?!]', '', x)\n",
    "    # Remove empty characters at the beginning and end\n",
    "    x = x.strip()\n",
    "    return x\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    # string.punctuation\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "def dataPreprocessing_w_contract_punct_remove(x):\n",
    "    # Convert words to lowercase\n",
    "    x = x.lower()\n",
    "    # Remove HTML\n",
    "    x = removeHTML(x)\n",
    "    # Delete strings starting with @\n",
    "    x = re.sub(\"@\\w+\", '',x)\n",
    "    # Delete Numbers\n",
    "    x = re.sub(\"'\\d+\", '',x)\n",
    "    x = re.sub(\"\\d+\", '',x)\n",
    "    # Delete URL\n",
    "    x = re.sub(\"http\\w+\", '',x)\n",
    "    # Replace consecutive empty spaces with a single space character\n",
    "    x = re.sub(r\"\\s+\", \" \", x)\n",
    "    x = expandContractions(x)\n",
    "    # Replace consecutive commas and periods with one comma and period character\n",
    "    x = re.sub(r\"\\.+\", \".\", x)\n",
    "    x = re.sub(r\"\\,+\", \",\", x)\n",
    "    x = re.sub(r'[^\\w\\s.,;:\"\"''?!]', '', x)\n",
    "    x = remove_punctuation(x)\n",
    "    # Remove empty characters at the beginning and end\n",
    "    x = x.strip()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paragraph based feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T23:18:52.401483Z",
     "iopub.status.busy": "2024-05-30T23:18:52.401306Z",
     "iopub.status.idle": "2024-05-30T23:19:08.373926Z",
     "shell.execute_reply": "2024-05-30T23:19:08.372999Z",
     "shell.execute_reply.started": "2024-05-30T23:18:52.401463Z"
    }
   },
   "outputs": [],
   "source": [
    "columns = [(pl.col(\"full_text\").str.split(by=\"\\n\\n\").alias(\"paragraph\"))]\n",
    "train = pl.from_pandas(train).with_columns(columns)\n",
    "test = pl.from_pandas(test).with_columns(columns)\n",
    "# paragraph features\n",
    "def Paragraph_Preprocess(tmp):\n",
    "    # Expand the paragraph list into several lines of data\n",
    "    tmp = tmp.explode('paragraph')\n",
    "    # Paragraph preprocessing\n",
    "    tmp = tmp.with_columns(pl.col('paragraph').map_elements(dataPreprocessing))\n",
    "    # Calculate the length of each paragraph\n",
    "    tmp = tmp.with_columns(pl.col('paragraph').map_elements(lambda x: len(x)).alias(\"paragraph_len\"))\n",
    "    # Calculate the number of sentences and words in each paragraph\n",
    "    tmp = tmp.with_columns(pl.col('paragraph').map_elements(lambda x: len(x.split('.'))).alias(\"paragraph_sentence_cnt\"),\n",
    "                    pl.col('paragraph').map_elements(lambda x: len(x.split(' '))).alias(\"paragraph_word_cnt\"),)\n",
    "    return tmp\n",
    "\n",
    "# feature_eng\n",
    "paragraph_fea = ['paragraph_len','paragraph_sentence_cnt','paragraph_word_cnt']\n",
    "def Paragraph_Eng(train_tmp):\n",
    "    aggs = [\n",
    "        # Count the number of paragraph lengths greater than and less than the i-value\n",
    "        *[pl.col('paragraph').filter(pl.col('paragraph_len') >= i).count().alias(f\"paragraph_{i}_cnt\") for i in [50,75,100,125,150,175,200,250,300,350,400,500,600,700] ], \n",
    "        *[pl.col('paragraph').filter(pl.col('paragraph_len') <= i).count().alias(f\"paragraph_{i}_cnt\") for i in [25,49]], \n",
    "        # other\n",
    "        *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in paragraph_fea],\n",
    "        *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in paragraph_fea],\n",
    "        *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in paragraph_fea],\n",
    "        *[pl.col(fea).first().alias(f\"{fea}_first\") for fea in paragraph_fea],\n",
    "        *[pl.col(fea).last().alias(f\"{fea}_last\") for fea in paragraph_fea],\n",
    "        *[pl.col(fea).sum().alias(f\"{fea}_sum\") for fea in paragraph_fea],\n",
    "        *[pl.col(fea).kurtosis().alias(f\"{fea}_kurtosis\") for fea in paragraph_fea],\n",
    "        *[pl.col(fea).quantile(0.25).alias(f\"{fea}_q1\") for fea in paragraph_fea],  \n",
    "        *[pl.col(fea).quantile(0.75).alias(f\"{fea}_q3\") for fea in paragraph_fea],\n",
    "    ]\n",
    "    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n",
    "    df = df.to_pandas()\n",
    "    return df\n",
    "\n",
    "tmp = Paragraph_Preprocess(train)\n",
    "train_feats = Paragraph_Eng(tmp)\n",
    "\n",
    "# Obtain feature names\n",
    "feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\n",
    "print('Features Number: ',len(feature_names))\n",
    "train_feats.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T23:19:08.375990Z",
     "iopub.status.busy": "2024-05-30T23:19:08.375402Z",
     "iopub.status.idle": "2024-05-30T23:19:24.038969Z",
     "shell.execute_reply": "2024-05-30T23:19:24.037990Z",
     "shell.execute_reply.started": "2024-05-30T23:19:08.375962Z"
    }
   },
   "outputs": [],
   "source": [
    "# sentence feature\n",
    "def Sentence_Preprocess(tmp):\n",
    "    # Preprocess full_text and use periods to segment sentences in the text\n",
    "    tmp = tmp.with_columns(pl.col('full_text').map_elements(dataPreprocessing).str.split(by=\".\").alias(\"sentence\"))\n",
    "    tmp = tmp.explode('sentence')\n",
    "    # Calculate the length of a sentence\n",
    "    tmp = tmp.with_columns(pl.col('sentence').map_elements(lambda x: len(x)).alias(\"sentence_len\"))\n",
    "    # Filter out the portion of data with a sentence length greater than 15\n",
    "    tmp = tmp.filter(pl.col('sentence_len')>=15)\n",
    "    # Count the number of words in each sentence\n",
    "    tmp = tmp.with_columns(pl.col('sentence').map_elements(lambda x: len(x.split(' '))).alias(\"sentence_word_cnt\"))\n",
    "    return tmp\n",
    "\n",
    "# feature_eng\n",
    "sentence_fea = ['sentence_len','sentence_word_cnt']\n",
    "def Sentence_Eng(train_tmp):\n",
    "    aggs = [\n",
    "        # Count the number of sentences with a length greater than i\n",
    "        *[pl.col('sentence').filter(pl.col('sentence_len') >= i).count().alias(f\"sentence_{i}_cnt\") for i in [15,50,100,150,200,250,300] ], \n",
    "        # other\n",
    "        *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in sentence_fea],\n",
    "        *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in sentence_fea],\n",
    "        *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in sentence_fea],\n",
    "        *[pl.col(fea).first().alias(f\"{fea}_first\") for fea in sentence_fea],\n",
    "        *[pl.col(fea).last().alias(f\"{fea}_last\") for fea in sentence_fea],\n",
    "        *[pl.col(fea).sum().alias(f\"{fea}_sum\") for fea in sentence_fea],\n",
    "        *[pl.col(fea).kurtosis().alias(f\"{fea}_kurtosis\") for fea in sentence_fea],\n",
    "        *[pl.col(fea).quantile(0.25).alias(f\"{fea}_q1\") for fea in sentence_fea], \n",
    "        *[pl.col(fea).quantile(0.75).alias(f\"{fea}_q3\") for fea in sentence_fea], \n",
    "        ]\n",
    "    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n",
    "    df = df.to_pandas()\n",
    "    return df\n",
    "\n",
    "tmp = Sentence_Preprocess(train)\n",
    "\n",
    "# Merge the newly generated feature data with the previously generated feature data\n",
    "train_feats = train_feats.merge(Sentence_Eng(tmp), on='essay_id', how='left')\n",
    "\n",
    "feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\n",
    "print('Features Number: ',len(feature_names))\n",
    "train_feats.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word based feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T23:19:24.040618Z",
     "iopub.status.busy": "2024-05-30T23:19:24.040384Z",
     "iopub.status.idle": "2024-05-30T23:19:44.632308Z",
     "shell.execute_reply": "2024-05-30T23:19:44.631441Z",
     "shell.execute_reply.started": "2024-05-30T23:19:24.040591Z"
    }
   },
   "outputs": [],
   "source": [
    "# word feature\n",
    "def Word_Preprocess(tmp):\n",
    "    # Preprocess full_text and use spaces to separate words from the text\n",
    "    tmp = tmp.with_columns(pl.col('full_text').map_elements(dataPreprocessing).str.split(by=\" \").alias(\"word\"))\n",
    "    tmp = tmp.explode('word')\n",
    "    # Calculate the length of each word\n",
    "    tmp = tmp.with_columns(pl.col('word').map_elements(lambda x: len(x)).alias(\"word_len\"))\n",
    "    # Delete data with a word length of 0\n",
    "    tmp = tmp.filter(pl.col('word_len')!=0)\n",
    "    \n",
    "    return tmp\n",
    "\n",
    "# feature_eng\n",
    "def Word_Eng(train_tmp):\n",
    "    aggs = [\n",
    "        # Count the number of words with a length greater than i+1\n",
    "        *[pl.col('word').filter(pl.col('word_len') >= i+1).count().alias(f\"word_{i+1}_cnt\") for i in range(15) ], \n",
    "        # other\n",
    "        pl.col('word_len').max().alias(f\"word_len_max\"),\n",
    "        pl.col('word_len').mean().alias(f\"word_len_mean\"),\n",
    "        pl.col('word_len').std().alias(f\"word_len_std\"),\n",
    "        pl.col('word_len').quantile(0.25).alias(f\"word_len_q1\"),\n",
    "        pl.col('word_len').quantile(0.50).alias(f\"word_len_q2\"),\n",
    "        pl.col('word_len').quantile(0.75).alias(f\"word_len_q3\"),\n",
    "        ]\n",
    "    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n",
    "    df = df.to_pandas()\n",
    "    return df\n",
    "\n",
    "tmp = Word_Preprocess(train)\n",
    "\n",
    "# Merge the newly generated feature data with the previously generated feature data\n",
    "train_feats = train_feats.merge(Word_Eng(tmp), on='essay_id', how='left')\n",
    "\n",
    "feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\n",
    "print('Features Number: ',len(feature_names))\n",
    "train_feats.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character TFIDF feature\n",
    "**TF (Term frequency)**: Number of time a term occur in a document / Total number of term in the document.\n",
    "\n",
    "**DF (Document frequency)**: Number of document where the term appear / Total number of document.\n",
    "\n",
    "**IDF (Inverse Document Frequency)**: 1 / Document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T23:19:44.634265Z",
     "iopub.status.busy": "2024-05-30T23:19:44.633980Z",
     "iopub.status.idle": "2024-05-30T23:21:06.958890Z",
     "shell.execute_reply": "2024-05-30T23:21:06.957978Z",
     "shell.execute_reply.started": "2024-05-30T23:19:44.634232Z"
    }
   },
   "outputs": [],
   "source": [
    "# TfidfVectorizer parameter\n",
    "vectorizer = TfidfVectorizer(\n",
    "            tokenizer=lambda x: x,\n",
    "            preprocessor=lambda x: x,\n",
    "            token_pattern=None,\n",
    "            strip_accents='unicode',\n",
    "            analyzer = 'word',\n",
    "            ngram_range=(1,3),\n",
    "            min_df=0.05,\n",
    "            max_df=0.95,\n",
    "            sublinear_tf=True,\n",
    ")\n",
    "# Fit all datasets into TfidfVector,this may cause leakage and overly optimistic CV scores\n",
    "train_tfid = vectorizer.fit_transform([i for i in train['full_text']])\n",
    "\n",
    "print(\"#\"*80)\n",
    "vect_feat_names=vectorizer.get_feature_names_out()\n",
    "print(vect_feat_names[100:110])\n",
    "print(\"#\"*80, \"\\n\\n\")\n",
    "\n",
    "# Convert to array\n",
    "dense_matrix = train_tfid.toarray()\n",
    "\n",
    "# Convert to dataframe\n",
    "df = pd.DataFrame(dense_matrix)\n",
    "\n",
    "# rename features\n",
    "tfid_columns = [ f'tfid_{i}' for i in range(len(df.columns))]\n",
    "df.columns = tfid_columns\n",
    "df['essay_id'] = train_feats['essay_id']\n",
    "\n",
    "# Merge the newly generated feature data with the previously generated feature data\n",
    "train_feats = train_feats.merge(df, on='essay_id', how='left')\n",
    "\n",
    "feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\n",
    "print('Features Number: ',len(feature_names))\n",
    "train_feats.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word TFIDF feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T23:21:06.960424Z",
     "iopub.status.busy": "2024-05-30T23:21:06.960220Z",
     "iopub.status.idle": "2024-05-30T23:21:28.753231Z",
     "shell.execute_reply": "2024-05-30T23:21:28.752406Z",
     "shell.execute_reply.started": "2024-05-30T23:21:06.960400Z"
    }
   },
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words('english')\n",
    "# TfidfVectorizer parameter\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    strip_accents='ascii',\n",
    "    analyzer = 'word',\n",
    "    ngram_range=(1,1),\n",
    "    min_df=0.15,\n",
    "    max_df=0.85,\n",
    "    sublinear_tf=True,\n",
    "    stop_words=stopwords_list,\n",
    ")\n",
    "# Fit all datasets into TfidfVector,this may cause leakage and overly optimistic CV scores\n",
    "processed_text = train.to_pandas()[\"full_text\"].progress_apply(lambda x: dataPreprocessing_w_contract_punct_remove(x))\n",
    "train_tfid = word_vectorizer.fit_transform([i for i in processed_text])\n",
    "\n",
    "# Convert to array\n",
    "dense_matrix = train_tfid.toarray()\n",
    "# Convert to dataframe\n",
    "df = pd.DataFrame(dense_matrix)\n",
    "# rename features\n",
    "tfid_w_columns = [ f'tfid_w_{i}' for i in range(len(df.columns))]\n",
    "df.columns = tfid_w_columns\n",
    "df['essay_id'] = train_feats['essay_id']\n",
    "\n",
    "df.head()\n",
    "# Merge the newly generated feature data with the previously generated feature data\n",
    "train_feats = train_feats.merge(df, on='essay_id', how='left')\n",
    "\n",
    "feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\n",
    "print('Features Number: ',len(feature_names))\n",
    "train_feats.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T23:21:28.755417Z",
     "iopub.status.busy": "2024-05-30T23:21:28.754753Z",
     "iopub.status.idle": "2024-05-30T23:22:49.735496Z",
     "shell.execute_reply": "2024-05-30T23:22:49.734624Z",
     "shell.execute_reply.started": "2024-05-30T23:21:28.755376Z"
    }
   },
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self) -> None:\n",
    "        self.twd = TreebankWordDetokenizer()\n",
    "        self.STOP_WORDS = set(stopwords.words('english'))\n",
    "        self.spellchecker = SpellChecker()\n",
    "\n",
    "    def spelling(self, text):\n",
    "        wordlist=text.split()\n",
    "        amount_miss = len(list(self.spellchecker.unknown(wordlist)))\n",
    "        return amount_miss\n",
    "    \n",
    "    def count_sym(self, text, sym):\n",
    "        sym_count = 0\n",
    "        for l in text:\n",
    "            if l == sym:\n",
    "                sym_count += 1\n",
    "        return sym_count\n",
    "\n",
    "    def run(self, data: pd.DataFrame, mode:str) -> pd.DataFrame:\n",
    "        \n",
    "        # preprocessing the text\n",
    "        data[\"processed_text\"] = data[\"full_text\"].apply(lambda x: dataPreprocessing_w_contract_punct_remove(x))\n",
    "        \n",
    "        # Text tokenization\n",
    "        data[\"text_tokens\"] = data[\"processed_text\"].apply(lambda x: word_tokenize(x))\n",
    "        \n",
    "        # essay length\n",
    "        data[\"text_length\"] = data[\"processed_text\"].apply(lambda x: len(x))\n",
    "        \n",
    "        # essay word count\n",
    "        data[\"word_count\"] = data[\"text_tokens\"].apply(lambda x: len(x))\n",
    "        \n",
    "        # essay unique word count\n",
    "        data[\"unique_word_count\"] = data[\"text_tokens\"].apply(lambda x: len(set(x)))\n",
    "        \n",
    "        # essay sentence count\n",
    "        data[\"sentence_count\"] = data[\"full_text\"].apply(lambda x: len(x.split('.')))\n",
    "        \n",
    "        # essay paragraph count\n",
    "        data[\"paragraph_count\"] = data[\"full_text\"].apply(lambda x: len(x.split('\\n\\n')))\n",
    "        \n",
    "        # count misspelling\n",
    "        data[\"splling_err_num\"] = data[\"processed_text\"].progress_apply(self.spelling)\n",
    "        print(\"Spelling mistake count done\")\n",
    "        \n",
    "        # ratio fullstop / text_length ** new\n",
    "        data[\"fullstop_ratio\"] = data[\"full_text\"].apply(lambda x: x.count(\".\")/len(x))\n",
    "        \n",
    "        # ratio comma / text_length ** new\n",
    "        data[\"comma_ratio\"] = data[\"full_text\"].apply(lambda x: x.count(\",\")/len(x))\n",
    "        \n",
    "        return data\n",
    "    \n",
    "preprocessor = Preprocessor()\n",
    "tmp = preprocessor.run(train.to_pandas(), mode=\"train\")\n",
    "train_feats = train_feats.merge(tmp, on='essay_id', how='left')\n",
    "feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\n",
    "print('Features Number: ',len(feature_names))\n",
    "train_feats.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test dataset featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T23:22:49.736906Z",
     "iopub.status.busy": "2024-05-30T23:22:49.736705Z",
     "iopub.status.idle": "2024-05-30T23:22:49.917886Z",
     "shell.execute_reply": "2024-05-30T23:22:49.917047Z",
     "shell.execute_reply.started": "2024-05-30T23:22:49.736882Z"
    }
   },
   "outputs": [],
   "source": [
    "# Paragraph\n",
    "tmp = Paragraph_Preprocess(test)\n",
    "test_feats = Paragraph_Eng(tmp)\n",
    "\n",
    "# Sentence\n",
    "tmp = Sentence_Preprocess(test)\n",
    "test_feats = test_feats.merge(Sentence_Eng(tmp), on='essay_id', how='left')\n",
    "\n",
    "# Word\n",
    "tmp = Word_Preprocess(test)\n",
    "test_feats = test_feats.merge(Word_Eng(tmp), on='essay_id', how='left')\n",
    "\n",
    "# Tfidf\n",
    "test_tfid = vectorizer.transform([i for i in test['full_text']])\n",
    "dense_matrix = test_tfid.toarray()\n",
    "df = pd.DataFrame(dense_matrix)\n",
    "tfid_columns = [ f'tfid_{i}' for i in range(len(df.columns))]\n",
    "df.columns = tfid_columns\n",
    "df['essay_id'] = test_feats['essay_id']\n",
    "test_feats = test_feats.merge(df, on='essay_id', how='left')\n",
    "\n",
    "# Word Tfidf\n",
    "processed_text = test.to_pandas()[\"full_text\"].progress_apply(lambda x: dataPreprocessing_w_contract_punct_remove(x))\n",
    "test_w_tfid = word_vectorizer.transform([i for i in processed_text])\n",
    "dense_matrix = test_w_tfid.toarray()\n",
    "df_w = pd.DataFrame(dense_matrix)\n",
    "tfid_w_columns = [ f'tfid_w_{i}' for i in range(len(df_w.columns))]\n",
    "df_w.columns = tfid_w_columns\n",
    "df_w['essay_id'] = test_feats['essay_id']\n",
    "test_feats = test_feats.merge(df_w, on='essay_id', how='left')\n",
    "\n",
    "# Extra feature\n",
    "tmp = preprocessor.run(test.to_pandas(), mode=\"train\")\n",
    "test_feats = test_feats.merge(tmp, on='essay_id', how='left')\n",
    "\n",
    "# Features number\n",
    "feature_names = list(filter(lambda x: x not in ['essay_id','score'], test_feats.columns))\n",
    "print('Features number: ',len(feature_names))\n",
    "test_feats.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T23:22:49.919217Z",
     "iopub.status.busy": "2024-05-30T23:22:49.918985Z",
     "iopub.status.idle": "2024-05-30T23:22:50.058284Z",
     "shell.execute_reply": "2024-05-30T23:22:50.057402Z",
     "shell.execute_reply.started": "2024-05-30T23:22:49.919192Z"
    }
   },
   "outputs": [],
   "source": [
    "## Add k-fold\n",
    "skf = StratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed)\n",
    "for i, (_, val_index) in enumerate(skf.split(train_feats, train_feats[\"score\"])):\n",
    "    train_feats.loc[val_index, \"fold\"] = i\n",
    "print(train_feats.shape)\n",
    "\n",
    "## feature selection\n",
    "target = \"score\"\n",
    "train_drop_columns = [\"essay_id\", \"fold\", \"full_text\", \"paragraph\", \"text_tokens\", \"processed_text\"] + [target]\n",
    "train_feats.drop(columns=train_drop_columns)\n",
    "test_drop_columns = [\"essay_id\", \"full_text\", \"paragraph\", \"text_tokens\", \"processed_text\"]\n",
    "test_feats.drop(columns=test_drop_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training LGBMRegressor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T23:22:50.059725Z",
     "iopub.status.busy": "2024-05-30T23:22:50.059510Z",
     "iopub.status.idle": "2024-05-30T23:22:50.068383Z",
     "shell.execute_reply": "2024-05-30T23:22:50.067569Z",
     "shell.execute_reply.started": "2024-05-30T23:22:50.059701Z"
    }
   },
   "outputs": [],
   "source": [
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    y_true = (y_true + a).round()\n",
    "    y_pred = (y_pred + a).clip(1, 6).round()\n",
    "    qwk = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n",
    "    return 'QWK', qwk, True\n",
    "\n",
    "def qwk_obj(y_true, y_pred):\n",
    "    labels = y_true + a\n",
    "    preds = y_pred + a\n",
    "    preds = preds.clip(1, 6)\n",
    "    f = 1/2*np.sum((preds-labels)**2)\n",
    "    g = 1/2*np.sum((preds-a)**2+b)\n",
    "    df = preds - labels\n",
    "    dg = preds - a\n",
    "    grad = (df/g - f*dg/g**2)*len(labels)\n",
    "    hess = np.ones(len(labels))\n",
    "    return grad, hess\n",
    "\n",
    "def qwk_param_calc(y):\n",
    "    a = y.mean()\n",
    "    b = (y ** 2).mean() - a**2\n",
    "    return np.round(a, 4), np.round(b, 4)\n",
    "# a = 2.948\n",
    "# b = 1.092"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T23:22:50.070444Z",
     "iopub.status.busy": "2024-05-30T23:22:50.069699Z",
     "iopub.status.idle": "2024-05-30T23:28:16.518640Z",
     "shell.execute_reply": "2024-05-30T23:28:16.517627Z",
     "shell.execute_reply.started": "2024-05-30T23:22:50.070411Z"
    }
   },
   "outputs": [],
   "source": [
    "LGBM_models = []\n",
    "\n",
    "callbacks = [\n",
    "    lgb.log_evaluation(period=25), \n",
    "    lgb.early_stopping(stopping_rounds=75,first_metric_only=True)\n",
    "]\n",
    "for fold in range(CFG.n_splits):\n",
    "\n",
    "    model = lgb.LGBMRegressor(\n",
    "        objective = qwk_obj, metrics = 'None', learning_rate = 0.1, max_depth = 5,\n",
    "        num_leaves = 10, colsample_bytree=0.5, reg_alpha = 0.1, reg_lambda = 0.8,\n",
    "        n_estimators=1024, random_state=CFG.seed, verbosity = - 1\n",
    "    )\n",
    "    \n",
    "    a, b = qwk_param_calc(train_feats[train_feats[\"fold\"] != fold][\"score\"])\n",
    "    \n",
    "    # Take out the training and validation sets for 5 kfold segmentation separately\n",
    "    X_train = train_feats[train_feats[\"fold\"] != fold].drop(columns=train_drop_columns)\n",
    "    y_train = train_feats[train_feats[\"fold\"] != fold][\"score\"] - a\n",
    "\n",
    "    X_eval = train_feats[train_feats[\"fold\"] == fold].drop(columns=train_drop_columns)\n",
    "    y_eval = train_feats[train_feats[\"fold\"] == fold][\"score\"] - a\n",
    "\n",
    "    print('\\nFold_{} Training ================================\\n'.format(fold+1))\n",
    "    print(f\"Fold {fold} a: {a}  ;;  b: {b}\")\n",
    "    # Training model\n",
    "    lgb_model = model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_names=['train', 'valid'],\n",
    "        eval_set=[(X_train, y_train), (X_eval, y_eval)],\n",
    "        eval_metric=quadratic_weighted_kappa,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    LGBM_models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T23:33:31.791723Z",
     "iopub.status.busy": "2024-05-30T23:33:31.791418Z",
     "iopub.status.idle": "2024-05-30T23:33:31.870125Z",
     "shell.execute_reply": "2024-05-30T23:33:31.869122Z",
     "shell.execute_reply.started": "2024-05-30T23:33:31.791691Z"
    }
   },
   "outputs": [],
   "source": [
    "lgbm_preds = []\n",
    "for fold, model in enumerate(LGBM_models):\n",
    "    X_eval_cv = test_feats.drop(columns=test_drop_columns)\n",
    "    pred = model.predict(X_eval_cv) + a\n",
    "    lgbm_preds.append(pred)\n",
    "\n",
    "print(lgbm_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T23:28:16.605067Z",
     "iopub.status.busy": "2024-05-30T23:28:16.604789Z",
     "iopub.status.idle": "2024-05-30T23:28:16.609108Z",
     "shell.execute_reply": "2024-05-30T23:28:16.608315Z",
     "shell.execute_reply.started": "2024-05-30T23:28:16.605039Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Combining the 5 model results\n",
    "# for i, pred in enumerate(preds):\n",
    "#     test_feats[f\"score_pred_{i}\"] = pred\n",
    "# test_feats[\"score\"] = np.round(test_feats[[f\"score_pred_{fold}\" for fold in range(CFG.n_splits)]].mean(axis=1),0).astype('int32')\n",
    "# test_feats[[\"essay_id\", \"score\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012248,
     "end_time": "2024-05-13T19:17:32.316881",
     "exception": false,
     "start_time": "2024-05-13T19:17:32.304633",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T23:17:04.787200Z",
     "iopub.status.busy": "2024-05-30T23:17:04.786719Z",
     "iopub.status.idle": "2024-05-30T23:17:05.544889Z",
     "shell.execute_reply": "2024-05-30T23:17:05.543960Z",
     "shell.execute_reply.started": "2024-05-30T23:17:04.787166Z"
    },
    "papermill": {
     "duration": 1.267719,
     "end_time": "2024-05-13T19:17:33.592756",
     "exception": false,
     "start_time": "2024-05-13T19:17:32.325037",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "\n",
    "import numpy as np, gc, re \n",
    "import pandas as pd \n",
    "\n",
    "train = pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv\")\n",
    "print(\"Train shape\",train.shape)\n",
    "display(train.head())\n",
    "print()\n",
    "\n",
    "test = pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv\")\n",
    "print(\"Test shape\",test.shape)\n",
    "display(test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.008867,
     "end_time": "2024-05-13T19:17:34.854859",
     "exception": false,
     "start_time": "2024-05-13T19:17:34.845992",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Generate Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.008934,
     "end_time": "2024-05-13T19:18:33.13384",
     "exception": false,
     "start_time": "2024-05-13T19:18:33.124906",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Combine Feature Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T23:28:16.610868Z",
     "iopub.status.busy": "2024-05-30T23:28:16.610565Z",
     "iopub.status.idle": "2024-05-30T23:28:16.702022Z",
     "shell.execute_reply": "2024-05-30T23:28:16.701340Z",
     "shell.execute_reply.started": "2024-05-30T23:28:16.610836Z"
    },
    "papermill": {
     "duration": 0.257046,
     "end_time": "2024-05-13T19:18:33.40012",
     "exception": false,
     "start_time": "2024-05-13T19:18:33.143074",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_train_embeds = np.concatenate(all_train_embeds,axis=1)\n",
    "all_test_embeds = np.concatenate(all_test_embeds,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T23:28:16.703354Z",
     "iopub.status.busy": "2024-05-30T23:28:16.703149Z",
     "iopub.status.idle": "2024-05-30T23:28:17.155705Z",
     "shell.execute_reply": "2024-05-30T23:28:17.154754Z",
     "shell.execute_reply.started": "2024-05-30T23:28:16.703331Z"
    }
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "print('Our concatenated train embeddings have shape', all_train_embeds.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct and Train MLP model\n",
    "## TODO: Refine and modify models and training process\n",
    "\n",
    "Link of original MLP model: https://www.kaggle.com/code/innotechryan/rapids-svr-starter-cv-0-830-lb-0-800/notebook\n",
    "\n",
    "## Initialize MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T23:28:17.157395Z",
     "iopub.status.busy": "2024-05-30T23:28:17.157143Z",
     "iopub.status.idle": "2024-05-30T23:28:17.351675Z",
     "shell.execute_reply": "2024-05-30T23:28:17.350684Z",
     "shell.execute_reply.started": "2024-05-30T23:28:17.157349Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# 检查 CUDA 是否可用\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1,hidden_size2, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# 设置超参数\n",
    "input_size = 5376  # 输入向量的维度\n",
    "hidden_size1 = 3200  # 隐藏层的大小\n",
    "hidden_size2 = 1600# 隐藏层的大小\n",
    "output_size = 6   # 输出类别的数量，这里假设为6个类别\n",
    "learning_rate = 0.001\n",
    "num_epochs = 8\n",
    "batch_size = 128\n",
    "n_split = 10\n",
    "\n",
    "# 初始化模型、损失函数和优化器\n",
    "model = MLP(input_size, hidden_size1,hidden_size2, output_size)\n",
    "# model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T23:28:17.357744Z",
     "iopub.status.busy": "2024-05-30T23:28:17.357469Z",
     "iopub.status.idle": "2024-05-30T23:28:17.962552Z",
     "shell.execute_reply": "2024-05-30T23:28:17.961455Z",
     "shell.execute_reply.started": "2024-05-30T23:28:17.357709Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# oof = np.zeros(len(train), dtype='float32')\n",
    "train = pd.read_csv(PATHS.train_path)\n",
    "test = pd.read_csv(PATHS.test_path)\n",
    "def comp_score(y_true,y_pred):\n",
    "    m = cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "    return m\n",
    "\n",
    "# 计算训练集和验证集的大小\n",
    "train_size = int(0.7 * len(train))\n",
    "valid_size = len(train) - train_size\n",
    "\n",
    "# 打乱数据集的索引顺序\n",
    "indices = np.random.permutation(len(train))\n",
    "\n",
    "# 根据比例划分训练集和验证集\n",
    "train_indices = indices[:train_size]\n",
    "valid_indices = indices[train_size:]\n",
    "\n",
    "    \n",
    "X_train = all_train_embeds[train_indices,]\n",
    "y_train = train.loc[train_indices,'score'].values\n",
    "X_valid = all_train_embeds[valid_indices,]\n",
    "y_valid = train.loc[valid_indices,'score'].values\n",
    "X_test = all_test_embeds\n",
    "X_train_tensor = torch.tensor(X_train)\n",
    "y_train_tensor = torch.tensor(y_train-1)\n",
    "X_valid_tensor = torch.tensor(X_valid)\n",
    "y_valid_tensor = torch.tensor(y_valid)\n",
    "\n",
    "# 创建 TensorDataset 和 DataLoader\n",
    "train_data = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_data = torch.utils.data.TensorDataset(X_valid_tensor, y_valid_tensor)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, shuffle=False)\n",
    "# train_MLP(model, criterion, optimizer, train_loader, num_epochs,X_valid_tensor,y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train process of 10 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T23:28:17.964204Z",
     "iopub.status.busy": "2024-05-30T23:28:17.963960Z",
     "iopub.status.idle": "2024-05-30T23:30:19.590822Z",
     "shell.execute_reply": "2024-05-30T23:30:19.589748Z",
     "shell.execute_reply.started": "2024-05-30T23:28:17.964176Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def comp_score(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "\n",
    "# 训练函数\n",
    "def train_MLP(model, criterion, optimizer, train_loader, num_epochs,X_valid_tensor,y_valid):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device),labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_valid_tensor = X_valid_tensor.to(device)\n",
    "            preds = torch.argmax(model(X_valid_tensor),dim=1)\n",
    "            score = comp_score(y_valid, (preds+1).cpu())    \n",
    "            epoch_loss = running_loss / len(train_loader.dataset)\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\"+f\"=> QWK score: {score}\")\n",
    "    return model\n",
    "\n",
    "# 初始化10折交叉验证\n",
    "kf = KFold(n_splits=n_split, shuffle=True, random_state=42)\n",
    "scores = []\n",
    "\n",
    "# 进行10折交叉验证\n",
    "for fold, (train_indices, valid_indices) in enumerate(kf.split(all_train_embeds)):\n",
    "    print(f'Fold {fold+1}')\n",
    "    \n",
    "    X_train = all_train_embeds[train_indices]\n",
    "    y_train = train.loc[train_indices, 'score'].values\n",
    "    X_valid = all_train_embeds[valid_indices]\n",
    "    y_valid = train.loc[valid_indices, 'score'].values\n",
    "    \n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train - 1, dtype=torch.long)\n",
    "    X_valid_tensor = torch.tensor(X_valid, dtype=torch.float32)\n",
    "    y_valid_tensor = torch.tensor(y_valid, dtype=torch.long)\n",
    "    \n",
    "    train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_data = TensorDataset(X_valid_tensor, y_valid_tensor)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # 重新初始化模型\n",
    "    model = MLP(input_size, hidden_size1,hidden_size2, output_size)  # 替换为你的模型类\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters())  # 根据需要选择优化器\n",
    "\n",
    "    # 训练模型\n",
    "    model = train_MLP(model, criterion, optimizer, train_loader, num_epochs, X_valid_tensor, y_valid_tensor)\n",
    "    \n",
    "    # 验证模型\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_valid_tensor = X_valid_tensor.to(device)\n",
    "        valid_preds = model(X_valid_tensor).argmax(dim=1).cpu().numpy()\n",
    "\n",
    "    model_path = f'/kaggle/working/model_fold_{fold+1}.pth'\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f'Model saved at {model_path}')\n",
    "    # 计算得分\n",
    "    score = comp_score(y_valid, valid_preds+1)\n",
    "    scores.append(score)\n",
    "    print(f'Score for fold {fold+1}: {score}')\n",
    "\n",
    "# 计算10折的平均得分\n",
    "mean_score = np.mean(scores)\n",
    "print(f'Mean score: {mean_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T23:32:00.500211Z",
     "iopub.status.busy": "2024-05-30T23:32:00.499661Z",
     "iopub.status.idle": "2024-05-30T23:32:02.888831Z",
     "shell.execute_reply": "2024-05-30T23:32:02.887868Z",
     "shell.execute_reply.started": "2024-05-30T23:32:00.500182Z"
    }
   },
   "outputs": [],
   "source": [
    "# 推理阶段加载模型并进行预测\n",
    "all_test_embeds_tensor = torch.tensor(all_test_embeds, dtype=torch.float32)\n",
    "all_test_embeds_tensor = all_test_embeds_tensor.to(device)\n",
    "mlp_preds = []\n",
    "\n",
    "for fold in range(n_split):\n",
    "    model = MLP(input_size, hidden_size1,hidden_size2, output_size)  # 替换为你的模型类\n",
    "    model.to(device)\n",
    "    model_path = f'/kaggle/working/model_fold_{fold+1}.pth'\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_preds = model(all_test_embeds_tensor).argmax(dim=1).cpu().numpy()\n",
    "    mlp_preds.append(test_preds+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T23:36:47.087156Z",
     "iopub.status.busy": "2024-05-30T23:36:47.086607Z",
     "iopub.status.idle": "2024-05-30T23:36:47.093607Z",
     "shell.execute_reply": "2024-05-30T23:36:47.092633Z",
     "shell.execute_reply.started": "2024-05-30T23:36:47.087121Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Prediction of LGBM models:\")\n",
    "print(lgbm_preds)\n",
    "print(\"Prediction of MLP models:\")\n",
    "print(mlp_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013187,
     "end_time": "2024-05-13T19:21:00.135815",
     "exception": false,
     "start_time": "2024-05-13T19:21:00.122628",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Create Submission CSV\n",
    "We average our 10 fold predictions and apply optimal thresholds above to convert the 10 sets of regression predictions into 1 set of final target predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine predictions from two type of models(15 models in total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T23:39:20.005427Z",
     "iopub.status.busy": "2024-05-30T23:39:20.005084Z",
     "iopub.status.idle": "2024-05-30T23:39:20.011610Z",
     "shell.execute_reply": "2024-05-30T23:39:20.010669Z",
     "shell.execute_reply.started": "2024-05-30T23:39:20.005396Z"
    }
   },
   "outputs": [],
   "source": [
    "# 计算每个样本的最终预测结果\n",
    "final_preds = np.mean(np.vstack((lgbm_preds, mlp_preds)), axis=0).round().astype(int)  # 这里采用平均方法\n",
    "print(final_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T23:39:33.661285Z",
     "iopub.status.busy": "2024-05-30T23:39:33.660708Z",
     "iopub.status.idle": "2024-05-30T23:39:33.666445Z",
     "shell.execute_reply": "2024-05-30T23:39:33.665474Z",
     "shell.execute_reply.started": "2024-05-30T23:39:33.661249Z"
    },
    "papermill": {
     "duration": 0.021494,
     "end_time": "2024-05-13T19:21:00.170466",
     "exception": false,
     "start_time": "2024-05-13T19:21:00.148972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_preds = final_preds\n",
    "print('Test preds shape:', test_preds.shape )\n",
    "print('First 3 test preds:',test_preds[:3] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T23:39:35.667699Z",
     "iopub.status.busy": "2024-05-30T23:39:35.667438Z",
     "iopub.status.idle": "2024-05-30T23:39:35.683690Z",
     "shell.execute_reply": "2024-05-30T23:39:35.682872Z",
     "shell.execute_reply.started": "2024-05-30T23:39:35.667669Z"
    },
    "papermill": {
     "duration": 0.023285,
     "end_time": "2024-05-13T19:21:00.207086",
     "exception": false,
     "start_time": "2024-05-13T19:21:00.183801",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/sample_submission.csv\")\n",
    "sub[\"score\"] = test_preds\n",
    "sub.score = sub.score.astype('int32')\n",
    "sub.to_csv(\"submission.csv\",index=False)\n",
    "print(\"Submission shape\", sub.shape )\n",
    "sub.head()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8059942,
     "sourceId": 71485,
     "sourceType": "competition"
    },
    {
     "datasetId": 5000565,
     "sourceId": 8403792,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5097492,
     "sourceId": 8534306,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 177473688,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 177475777,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 258.371239,
   "end_time": "2024-05-13T19:21:02.799575",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-13T19:16:44.428336",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
