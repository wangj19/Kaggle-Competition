{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":73231,"databundleVersionId":8365361,"sourceType":"competition"},{"sourceId":7369493,"sourceType":"datasetVersion","datasetId":4281572},{"sourceId":8012825,"sourceType":"datasetVersion","datasetId":4720595},{"sourceId":8023365,"sourceType":"datasetVersion","datasetId":4728129},{"sourceId":8052555,"sourceType":"datasetVersion","datasetId":4748944},{"sourceId":11261,"sourceType":"modelInstanceVersion","modelInstanceId":8332},{"sourceId":11264,"sourceType":"modelInstanceVersion","modelInstanceId":8318}],"dockerImageVersionId":30699,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":724.728315,"end_time":"2024-02-29T09:37:08.760349","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-02-29T09:25:04.032034","version":"2.5.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"21267b653022419eb6fc3f47aa4db8ed":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_926e7ccdad6440be85c76931860b744c","placeholder":"​","style":"IPY_MODEL_feef8334edb24f6da22e8bb1d8d80c67","value":"Loading checkpoint shards: 100%"}},"2144e851698b4707ad1c7fc29fe21b03":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3963993becfa487c9ff725f211915e67":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7a725e1b0cc4ad78a62beab5f663065","placeholder":"​","style":"IPY_MODEL_fdb32baaed7145d8a8024b615ef242ca","value":" 19/19 [10:48&lt;00:00, 33.24s/it]"}},"5882b6e860be4a0db012a64fc0704a3f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_21267b653022419eb6fc3f47aa4db8ed","IPY_MODEL_d91eb83d016a4381828192a98f798f9b","IPY_MODEL_3963993becfa487c9ff725f211915e67"],"layout":"IPY_MODEL_6a892a5561f742bb9db9f13859c18e90"}},"6a892a5561f742bb9db9f13859c18e90":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"926e7ccdad6440be85c76931860b744c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d91eb83d016a4381828192a98f798f9b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2144e851698b4707ad1c7fc29fe21b03","max":19,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e0693b32889c42b18b9a3844e045d048","value":19}},"e0693b32889c42b18b9a3844e045d048":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f7a725e1b0cc4ad78a62beab5f663065":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fdb32baaed7145d8a8024b615ef242ca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"feef8334edb24f6da22e8bb1d8d80c67":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}},"version_major":2,"version_minor":0}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\n## Start code Information\nstarter code is forked from https://www.kaggle.com/code/abdurrafae/improved-code-interpretation\n\ncredits:\n\nhttps://www.kaggle.com/code/olyatsimboy/aimo-openmath-mistral-baseline\n\nhttps://www.kaggle.com/code/aatiffraz/prompt-prediction-w-mixtral-mistral7b-gemma-llama\n\nhttps://www.kaggle.com/code/thedrcat/aimo-mixtral-baseline","metadata":{}},{"cell_type":"code","source":"import sys\nprint('python版本：',sys.version)\n\nimport pkg_resources\n\ndef get_package_version(package_name):\n    try:\n        version = pkg_resources.get_distribution(package_name).version\n        return version\n    except pkg_resources.DistributionNotFound:\n        return \"Package not found\"\n\npackage_name = \"torch\"\nversion = get_package_version(package_name)\nprint(f\"{package_name}版本：{version}\")\n\nimport torch\n\ncuda_version = torch.version.cuda\nprint(\"CUDA版本：\", cuda_version)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T20:31:37.293150Z","iopub.execute_input":"2024-06-08T20:31:37.293819Z","iopub.status.idle":"2024-06-08T20:31:41.490461Z","shell.execute_reply.started":"2024-06-08T20:31:37.293778Z","shell.execute_reply":"2024-06-08T20:31:41.489499Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"python版本： 3.10.13 | packaged by conda-forge | (main, Dec 23 2023, 15:36:39) [GCC 12.3.0]\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_34/1834013600.py:4: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n  import pkg_resources\n","output_type":"stream"},{"name":"stdout","text":"torch版本：2.1.2\nCUDA版本： 12.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import time\n\nNOTEBOOK_START_TIME = time.time()","metadata":{"execution":{"iopub.status.busy":"2024-06-08T20:31:46.483618Z","iopub.execute_input":"2024-06-08T20:31:46.484857Z","iopub.status.idle":"2024-06-08T20:31:46.489732Z","shell.execute_reply.started":"2024-06-08T20:31:46.484811Z","shell.execute_reply":"2024-06-08T20:31:46.488601Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import aimo\n\nenv = aimo.make_env()\niter_test = env.iter_test()","metadata":{"execution":{"iopub.status.busy":"2024-06-08T20:31:46.805496Z","iopub.execute_input":"2024-06-08T20:31:46.805861Z","iopub.status.idle":"2024-06-08T20:31:47.199581Z","shell.execute_reply.started":"2024-06-08T20:31:46.805834Z","shell.execute_reply":"2024-06-08T20:31:47.198804Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"TO-DO\n\nChange temperature as the question goes longer\nChange temperature based on question lenght","metadata":{}},{"cell_type":"markdown","source":"# Model\n## Zero-shot MMOS-DeepSeekMath-7B with self-consistency and generated code reasoning evaluation\n\nSelf-consistency is a modification of the standard greedy decoding in reasoning pipelines via sampling several diverse answers followed by aggregation, e.g., most common answer ([SC-CoT paper](https://arxiv.org/pdf/2203.11171.pdf)).\n\nIn this kernel, we will consider MMOS-DeepSeekMath-7B RL-tuned backbone; in my experiments, this model produces more consistent code reasoning and the code block execution will allow us to decrease arithmetic hallucinations.","metadata":{}},{"cell_type":"code","source":"DEBUG = False\n\nQUANT = False\n\nif QUANT:\n    from transformers import BitsAndBytesConfig\n    quantization_config = BitsAndBytesConfig(\n        load_in_4bit = True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.bfloat16,\n        bnb_4bit_use_double_quant=True,\n    )\n\nUSE_PAST_KEY = True","metadata":{"papermill":{"duration":18.075198,"end_time":"2024-02-29T09:25:25.295954","exception":false,"start_time":"2024-02-29T09:25:07.220756","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-08T20:32:05.667330Z","iopub.execute_input":"2024-06-08T20:32:05.668242Z","iopub.status.idle":"2024-06-08T20:32:05.675512Z","shell.execute_reply.started":"2024-06-08T20:32:05.668210Z","shell.execute_reply":"2024-06-08T20:32:05.674456Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"%%time\nif QUANT:\n    !pip install -U /kaggle/input/accelerate-wheelwhl/accelerate-0.29.1-py3-none-any.whl -qq\n    !pip install -U /kaggle/input/bitsandbytes-0-42-0-py3-none-any-whl/bitsandbytes-0.42.0-py3-none-any.whl -qq\n\n\nimport torch\nimport gc\ntorch.backends.cuda.enable_mem_efficient_sdp(False)\n\nfrom transformers import (\n    AutoModelForCausalLM, \n    AutoTokenizer, \n    AutoConfig,\n    StoppingCriteria,\n    set_seed\n)\n\nimport transformers\nprint(f\"Transformers Version: {transformers.__version__}\")\nset_seed(42)","metadata":{"papermill":{"duration":18.075198,"end_time":"2024-02-29T09:25:25.295954","exception":false,"start_time":"2024-02-29T09:25:07.220756","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-08T20:32:06.445390Z","iopub.execute_input":"2024-06-08T20:32:06.445782Z","iopub.status.idle":"2024-06-08T20:32:20.170173Z","shell.execute_reply.started":"2024-06-08T20:32:06.445749Z","shell.execute_reply":"2024-06-08T20:32:20.169267Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Transformers Version: 4.39.3\n","output_type":"stream"},{"name":"stderr","text":"2024-06-08 20:32:11.074449: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-08 20:32:11.074599: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-08 20:32:11.234675: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"CPU times: user 7.07 s, sys: 1.16 s, total: 8.23 s\nWall time: 13.7 s\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom tqdm import tqdm\nPRIVATE = True\n\n# df = pd.read_csv('/kaggle/input/ai-mathematical-olympiad-prize/test.csv')\n# df.head()","metadata":{"papermill":{"duration":1.224774,"end_time":"2024-02-29T09:36:31.21757","exception":false,"start_time":"2024-02-29T09:36:29.992796","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-08T20:32:21.663855Z","iopub.execute_input":"2024-06-08T20:32:21.664437Z","iopub.status.idle":"2024-06-08T20:32:21.669244Z","shell.execute_reply.started":"2024-06-08T20:32:21.664409Z","shell.execute_reply":"2024-06-08T20:32:21.668122Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# if len(df) < 5:\n#     df = pd.read_csv('/kaggle/input/ai-mathematical-olympiad-prize/train.csv')\n#     PRIVATE = False\n# df.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-08T20:33:19.074249Z","iopub.execute_input":"2024-06-08T20:33:19.074948Z","iopub.status.idle":"2024-06-08T20:33:19.079104Z","shell.execute_reply.started":"2024-06-08T20:33:19.074895Z","shell.execute_reply":"2024-06-08T20:33:19.078009Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def naive_parse(answer):\n    out = []\n    start = False\n    end = False\n    for l in reversed(list(answer)):\n        if l in '0123456789' and not end:\n            start = True\n            out.append(l)\n        else:\n            if start:\n                end = True\n        \n    out = reversed(out)\n    return ''.join(out)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T20:33:19.786271Z","iopub.execute_input":"2024-06-08T20:33:19.786959Z","iopub.status.idle":"2024-06-08T20:33:19.793070Z","shell.execute_reply.started":"2024-06-08T20:33:19.786926Z","shell.execute_reply":"2024-06-08T20:33:19.792092Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import re\nimport sys\nimport subprocess\n\ndef return_last_print(output, n):\n    lines = output.strip().split('\\n')\n    if lines:\n        return lines[n]\n    else:\n        return \"\"\n\ndef process_code(code, return_shell_output=False):\n    \n    def repl(match):\n        if \"real\" not in match.group():\n            return \"{}{}\".format(match.group()[:-1], ', real=True)')\n        else:\n            return \"{}{}\".format(match.group()[:-1], ')')\n    code = re.sub(r\"symbols\\([^)]+\\)\", repl, code)\n\n    if return_shell_output:\n        code = code.replace('\\n', '\\n    ')\n            # Add a try...except block\n        code = \"\\ntry:\\n    from sympy import *\\n{}\\nexcept Exception as e:\\n    print(e)\\n    print('FAIL')\\n\".format(code)\n    \n    if not return_shell_output:\n        print(code)\n    with open('code.py', 'w') as fout:\n        fout.write(code)\n    \n    batcmd = 'timeout 7 ' + sys.executable + ' code.py'\n    try:\n        shell_output = subprocess.check_output(batcmd, shell=True).decode('utf8')\n        return_value = return_last_print(shell_output, -1)\n        print(shell_output)\n        if return_shell_output:\n            if return_value=='FAIL':\n                CODE_STATUS = False\n                return_value = return_last_print(shell_output, -2)\n                if \"not defined\" in return_value:\n                    return_value+='\\nTry checking the formatting and imports'\n            else:\n                CODE_STATUS = True\n            return return_value, CODE_STATUS  \n        code_output = round(float(eval(return_value))) % 1000\n    except Exception as e:\n        print(e,'shell_output')\n        code_output = -1\n    \n    if return_shell_output:\n        if code_output==-1:\n            CODE_STATUS = False\n        else:\n            CODE_STATUS = True\n        return code_output, CODE_STATUS  \n    \n    \n    return code_output\n\n\ndef process_text_output(output):\n    result = output    \n    try:\n        result_output = re.findall(r'\\\\boxed\\{(\\d+)\\}', result)\n\n        print('BOXED', result_output)\n        if not len(result_output):\n            result_output = naive_parse(result)\n        else:\n            result_output = result_output[-1]\n\n        print('BOXED FINAL', result_output)\n        if not len(result_output):\n            result_output = -1\n        \n        else:\n            result_output = round(float(eval(result_output))) % 1000\n    \n    except Exception as e:\n        print(e)\n        print('ERROR PARSING TEXT')\n        result_output = -1\n    \n    return result_output\n","metadata":{"execution":{"iopub.status.busy":"2024-06-08T20:33:21.598341Z","iopub.execute_input":"2024-06-08T20:33:21.599044Z","iopub.status.idle":"2024-06-08T20:33:21.614596Z","shell.execute_reply.started":"2024-06-08T20:33:21.599010Z","shell.execute_reply":"2024-06-08T20:33:21.613615Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-06-08T20:33:22.552749Z","iopub.execute_input":"2024-06-08T20:33:22.553616Z","iopub.status.idle":"2024-06-08T20:33:22.783708Z","shell.execute_reply.started":"2024-06-08T20:33:22.553566Z","shell.execute_reply":"2024-06-08T20:33:22.782836Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"import re\nimport math\nimport random\n\nfrom collections import defaultdict\n\nn_repetitions = 17 if PRIVATE else 4 # Original notebook had 22 but times out :(\nTOTAL_TOKENS = 2048 # if PRIVATE else 512\n\nif PRIVATE:\n    TIME_LIMIT = 31500\nelse:\n    TIME_LIMIT = 1","metadata":{"execution":{"iopub.status.busy":"2024-06-08T20:33:23.152035Z","iopub.execute_input":"2024-06-08T20:33:23.152687Z","iopub.status.idle":"2024-06-08T20:33:23.157544Z","shell.execute_reply.started":"2024-06-08T20:33:23.152656Z","shell.execute_reply":"2024-06-08T20:33:23.156582Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"if PRIVATE:\n\n    MODEL_PATH = \"/kaggle/input/deepseek-math\"#\"/kaggle/input/gemma/transformers/7b-it/1\"\n    DEEP = True\n\n    config = AutoConfig.from_pretrained(MODEL_PATH)\n    config.gradient_checkpointing = True\n\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n\n    device_map = [('model.embed_tokens', 0),\n                 ('model.layers.0', 0),\n                 ('model.layers.1', 0),\n                 ('model.layers.2', 0),\n                 ('model.layers.3', 0),\n                 ('model.layers.4', 0),\n                 ('model.layers.5', 0),\n                 ('model.layers.6', 0),\n                 ('model.layers.7', 0),\n                 ('model.layers.8', 0),\n                 ('model.layers.9', 0),\n                 ('model.layers.10', 0),\n                 ('model.layers.11', 0),\n                 ('model.layers.12', 0),\n                 ('model.layers.13', 0),\n                 ('model.layers.14', 0),\n                 ('model.layers.15', 0),\n                 ('model.layers.16', 0),\n                 ('model.layers.17', 0),\n                 ('model.layers.18', 0),\n                 ('model.layers.19', 0),\n                 ('model.layers.20', 0),\n                 ('model.layers.21', 0),\n                 ('model.layers.22', 1),\n                 ('model.layers.23', 1),\n                 ('model.layers.24', 1),\n                 ('model.layers.25', 1),\n                 ('model.layers.26', 1),\n                 ('model.layers.27', 1),\n                 ('model.layers.28', 1),\n                 ('model.layers.29', 1),\n                 ('model.norm', 1),\n                 ('lm_head', 1)]\n\n    device_map = {ii:jj for (ii,jj) in device_map}\n\n    if QUANT:\n        from transformers import BitsAndBytesConfig\n        quantization_config = BitsAndBytesConfig(\n            load_in_4bit = True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch.bfloat16,\n            bnb_4bit_use_double_quant=True,\n        )\n        model = AutoModelForCausalLM.from_pretrained(\n            MODEL_PATH,\n            device_map=\"sequential\",\n            torch_dtype=\"auto\",\n            trust_remote_code=True, \n            quantization_config=quantization_config,\n            config=config\n        )\n    else:  \n        model = AutoModelForCausalLM.from_pretrained(\n            MODEL_PATH,\n            device_map=device_map,\n            torch_dtype=\"auto\",\n            trust_remote_code=True,\n            #quantization_config=quantization_config,\n            config=config\n        )\n    \n    pipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype='auto',\n    device_map=device_map,\n)\n    from transformers import StoppingCriteriaList\n\n    class StoppingCriteriaSub(StoppingCriteria):\n        def __init__(self, stops = [], encounters=1):\n            super().__init__()\n            self.stops = [stop.to(\"cuda\") for stop in stops]\n\n        def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n            for stop in self.stops:\n                last_token = input_ids[0][-len(stop):]\n                if torch.all(torch.eq(stop,last_token)):\n                    return True\n            return False\n\n\n    stop_words = [\"```output\", \"```python\", \"```\\nOutput\" , \")\\n```\" , \"``````output\"] #,  \n    stop_words_ids = [tokenizer(stop_word, return_tensors='pt', add_special_tokens=False)['input_ids'].squeeze() for stop_word in stop_words]\n    stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words_ids)])\n    \n    model.dtype, model.hf_device_map","metadata":{"execution":{"iopub.status.busy":"2024-06-08T20:33:23.986458Z","iopub.execute_input":"2024-06-08T20:33:23.986807Z","iopub.status.idle":"2024-06-08T20:35:57.877860Z","shell.execute_reply.started":"2024-06-08T20:33:23.986782Z","shell.execute_reply":"2024-06-08T20:35:57.877026Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"add23fa43d3747a3a2e01ca6d47f9c48"}},"metadata":{}}]},{"cell_type":"code","source":"code = \"\"\"Below is a math problem you are to solve (positive numerical answer):\n\\\"{}\\\"\nTo accomplish this, first determine a sympy-based approach for solving the problem by listing each step to take and what functions need to be called in each step. Be clear so even an idiot can follow your instructions, and remember, your final answer should be positive integer, not an algebraic expression!\nWrite the entire script covering all the steps (use comments and document it well) and print the result. After solving the problem, output the final numerical answer within \\\\boxed{}.\n\nApproach:\"\"\"\n\n\ncot = \"\"\"Below is a math problem you are to solve (positive numerical answer!):\n\\\"{}\\\"\nAnalyze this problem and think step by step to come to a solution with programs. After solving the problem, output the final numerical answer within \\\\boxed{}.\\n\\n\"\"\"\n\npromplt_options = [code,cot]","metadata":{"execution":{"iopub.status.busy":"2024-06-08T20:37:52.035458Z","iopub.execute_input":"2024-06-08T20:37:52.035770Z","iopub.status.idle":"2024-06-08T20:37:52.041147Z","shell.execute_reply.started":"2024-06-08T20:37:52.035744Z","shell.execute_reply":"2024-06-08T20:37:52.040228Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"import re\nfrom collections import defaultdict\nfrom collections import Counter\n\nfrom numpy.random import choice\nimport numpy as np\n\ntool_instruction = '\\n\\nPlease integrate natural language reasoning with programs to solve the above problem, and put your final numerical answer within \\\\boxed{}.\\nNote that the intermediary calculations may be real numbers, but the final numercal answer would always be an integer.'\n\n\n#tool_instruction = \" The answer should be given as a non-negative modulo 1000.\"\n#tool_instruction += '\\nPlease integrate natural language reasoning with programs to solve the problem above, and put your final answer within \\\\boxed{}.'\n\ntemperature = 0.9\ntop_p = 1.0\n\ntemperature_coding = 0.9\ntop_p_coding = 1.0\n\n   \ntotal_results = {}\ntotal_answers = {}\nbest_stats = {}\ntotal_outputs = {}\nquestion_type_counts = {}\nstarting_counts = (2,3)\n    \nfor i, (test, sample_submission) in tqdm(enumerate(iter_test)):\n    print(f\"Solving problem {i} ...\")\n    TIME_SPENT = time.time() - NOTEBOOK_START_TIME\n\n    if TIME_SPENT>TIME_LIMIT:\n        sample_submission['answer'] = 0\n        env.predict(sample_submission)\n        break\n        \n    for jj in tqdm(range(n_repetitions)):   \n        problem = test['problem'].values[0]\n        print(f\"\\n\\n\\nQUESTION {i} - {jj} - TIME_SPENT : {TIME_SPENT:.0f} secs\")\n        \n        best, best_count = best_stats.get(i,(-1,-1))\n        if best_count>np.sqrt(jj):\n            print(\"SKIPPING CAUSE ALREADY FOUND BEST\")\n            continue\n            \n        outputs = total_outputs.get(i,[])\n        text_answers, code_answers = question_type_counts.get(i,starting_counts)\n        results = total_results.get(i,[])\n        answers = total_answers.get(i,[])\n        \n        for _ in range(5):\n            torch.cuda.empty_cache()\n            gc.collect()\n            time.sleep(0.2)\n\n        try:\n            ALREADY_GEN = 0\n            code_error = None\n            code_error_count = 0\n            code_output = -1\n            #initail_message = problem  + tool_instruction \n            counts = np.array([text_answers,code_answers])\n\n            draw = choice(promplt_options, 1,\n                          p=counts/counts.sum())\n\n            initail_message = draw[0].format(problem,\"{}\")            \n            prompt = f\"User: {initail_message}\"\n\n            current_printed = len(prompt)\n            print(f\"{jj}_{prompt}\\n\")\n\n            model_inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n            input_len = len(model_inputs['input_ids'][0])\n\n            generation_output = model.generate(**model_inputs, \n                                               max_new_tokens=TOTAL_TOKENS-ALREADY_GEN,\n                                               return_dict_in_generate=USE_PAST_KEY,\n                                               do_sample = True,\n                                               temperature = temperature,\n                                               top_p = top_p,\n                                               num_return_sequences=1, stopping_criteria = stopping_criteria)\n\n            if USE_PAST_KEY:\n                output_ids = generation_output.sequences[0]\n            else:\n                output_ids = generation_output[0]\n            decoded_output = tokenizer.decode(output_ids, skip_special_tokens=True)\n            print(f\"{decoded_output[current_printed:]}\\n\")\n            current_printed += len(decoded_output[current_printed:])\n            cummulative_code = \"\"\n            \n            \n            stop_word_cond = False\n            for stop_word in stop_words:\n                stop_word_cond = stop_word_cond or (decoded_output[-len(stop_word):]==stop_word)\n                \n            \n            while (stop_word_cond) and (ALREADY_GEN<(TOTAL_TOKENS)):\n\n                if (decoded_output[-len(\"```python\"):]==\"```python\"):\n                    temperature_inner=temperature_coding\n                    top_p_inner = top_p_coding\n                    prompt = decoded_output\n                else:\n                    temperature_inner=temperature\n                    top_p_inner = top_p\n                    try:\n                        if (decoded_output[-len(\"``````output\"):]==\"``````output\"):\n                            code_text = decoded_output.split('```python')[-1].split(\"``````\")[0]\n                        else:\n                            code_text = decoded_output.split('```python')[-1].split(\"```\")[0]\n                        \n\n                        cummulative_code+=code_text\n                        code_output, CODE_STATUS = process_code(cummulative_code, return_shell_output=True)\n                        print('CODE RESULTS', code_output)\n\n                        if code_error==code_output:\n                            code_error_count+=1\n                        else:\n                            code_error=code_output\n                            code_error_count = 0\n\n                        if not CODE_STATUS:\n                            cummulative_code = cummulative_code[:-len(code_text)]\n\n                            if code_error_count>=1:\n                                print(\"REPEATED ERRORS\")\n                                break\n\n                    except Exception as e:\n                        print(e)\n                        print('ERROR PARSING CODE')\n                        code_output = -1\n\n                    if code_output!=-1:\n                        if (decoded_output[-len(\")\\n```\"):]==\")\\n```\"):\n                            prompt = decoded_output+'```output\\n'+str(code_output)+'\\n```\\n'\n                        else:\n                            prompt = decoded_output+'\\n'+str(code_output)+'\\n```\\n'\n                    else:\n                        prompt = decoded_output\n                        cummulative_code=\"\"\n\n\n                model_inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n                ALREADY_GEN =  len(model_inputs['input_ids'][0])-input_len\n\n                if USE_PAST_KEY:\n                    old_values = generation_output.past_key_values\n                else:\n                    old_values = None\n\n                generation_output = model.generate(**model_inputs, \n                                                   max_new_tokens=TOTAL_TOKENS-ALREADY_GEN, \n                                                   return_dict_in_generate=USE_PAST_KEY,\n                                                   past_key_values=old_values,\n                                                   do_sample = True,\n                                                   temperature = temperature_inner,\n                                                   top_p = top_p_inner,\n                                                   num_return_sequences=1, stopping_criteria = stopping_criteria)\n\n                if USE_PAST_KEY:\n                    output_ids = generation_output.sequences[0]\n                else:\n                    output_ids = generation_output[0]\n                decoded_output = tokenizer.decode(output_ids, skip_special_tokens=True)\n                print(f\"\\nINTERMEDIATE OUT :\\n{decoded_output[current_printed:]}\\n\")\n                current_printed+=len(decoded_output[current_printed:])\n                \n                stop_word_cond = False\n                for stop_word in stop_words:\n                    stop_word_cond = stop_word_cond or (decoded_output[-len(stop_word):]==stop_word)\n\n            if USE_PAST_KEY:\n                output_ids = generation_output.sequences[0]\n            else:\n                output_ids = generation_output[0]\n\n            raw_output = tokenizer.decode(output_ids[input_len:], skip_special_tokens=True)\n            #print(f\"\\n\\nOutput :\\n{raw_output}\\n\")                            \n            result_output = process_text_output(raw_output)\n            \n            try:\n                code_output = round(float(eval(code_output))) % 1000\n            except Exception as e:\n                print(e,'final_eval')\n                code_output = -1\n\n        except Exception as e:\n            print(e,\"5\")\n            result_output, code_output = -1, -1\n\n        if code_output!=-1:\n            outputs.append(code_output)\n            code_answers+=1\n\n        if result_output!=-1:\n            outputs.append(result_output)\n            text_answers+=1\n\n        if len(outputs) > 0:\n            occurances = Counter(outputs).most_common()\n            print(occurances)\n            if occurances[0][1] > best_count:\n                print(\"GOOD ANSWER UPDATED!\")\n                best = occurances[0][0]\n                best_count = occurances[0][1]\n            if occurances[0][1] > 5:\n                print(\"ANSWER FOUND!\")\n                break\n\n        results.append(result_output)\n        answers.append(code_output)\n        \n        best_stats[i] = (best, best_count) \n        question_type_counts[i] = (text_answers, code_answers)\n        total_outputs[i] = outputs\n        \n        total_results[i] = results\n        total_answers[i] = answers\n\n        print(\"code_answers\",code_answers-starting_counts[1],\"text_answers\",text_answers-starting_counts[0])\n        if DEBUG:\n            break\n            \n    print(f\"Predicted best answer: {best_stats}\")\n    sample_submission['answer'] = best_stats[i][0]\n    env.predict(sample_submission)","metadata":{"papermill":{"duration":34.259365,"end_time":"2024-02-29T09:37:05.548829","exception":false,"start_time":"2024-02-29T09:36:31.289464","status":"completed"},"tags":[],"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-06-08T20:35:57.879485Z","iopub.execute_input":"2024-06-08T20:35:57.879807Z","iopub.status.idle":"2024-06-08T20:37:52.034001Z","shell.execute_reply.started":"2024-06-08T20:35:57.879782Z","shell.execute_reply":"2024-06-08T20:37:52.032978Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\nSolving problem 0 ...\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/17 [00:00<?, ?it/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"\n\n\nQUESTION 0 - 0 - TIME_SPENT : 251 secs\n","output_type":"stream"},{"name":"stderr","text":"\n  6%|▌         | 1/17 [00:02<00:36,  2.26s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"name 'promplt_options' is not defined 5\ncode_answers 0 text_answers 0\n\n\n\nQUESTION 0 - 1 - TIME_SPENT : 251 secs\n","output_type":"stream"},{"name":"stderr","text":"\n 12%|█▏        | 2/17 [00:04<00:33,  2.24s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"name 'promplt_options' is not defined 5\ncode_answers 0 text_answers 0\n\n\n\nQUESTION 0 - 2 - TIME_SPENT : 251 secs\n","output_type":"stream"},{"name":"stderr","text":"\n 18%|█▊        | 3/17 [00:06<00:31,  2.22s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"name 'promplt_options' is not defined 5\ncode_answers 0 text_answers 0\n\n\n\nQUESTION 0 - 3 - TIME_SPENT : 251 secs\n","output_type":"stream"},{"name":"stderr","text":"\n 24%|██▎       | 4/17 [00:08<00:28,  2.22s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"name 'promplt_options' is not defined 5\ncode_answers 0 text_answers 0\n\n\n\nQUESTION 0 - 4 - TIME_SPENT : 251 secs\n","output_type":"stream"},{"name":"stderr","text":"\n 29%|██▉       | 5/17 [00:11<00:26,  2.24s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"name 'promplt_options' is not defined 5\ncode_answers 0 text_answers 0\n\n\n\nQUESTION 0 - 5 - TIME_SPENT : 251 secs\n","output_type":"stream"},{"name":"stderr","text":"\n 35%|███▌      | 6/17 [00:13<00:24,  2.24s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"name 'promplt_options' is not defined 5\ncode_answers 0 text_answers 0\n\n\n\nQUESTION 0 - 6 - TIME_SPENT : 251 secs\n","output_type":"stream"},{"name":"stderr","text":"\n 41%|████      | 7/17 [00:15<00:22,  2.24s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"name 'promplt_options' is not defined 5\ncode_answers 0 text_answers 0\n\n\n\nQUESTION 0 - 7 - TIME_SPENT : 251 secs\n","output_type":"stream"},{"name":"stderr","text":"\n 47%|████▋     | 8/17 [00:17<00:20,  2.23s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"name 'promplt_options' is not defined 5\ncode_answers 0 text_answers 0\n\n\n\nQUESTION 0 - 8 - TIME_SPENT : 251 secs\n","output_type":"stream"},{"name":"stderr","text":"\n 53%|█████▎    | 9/17 [00:20<00:17,  2.24s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"name 'promplt_options' is not defined 5\ncode_answers 0 text_answers 0\n\n\n\nQUESTION 0 - 9 - TIME_SPENT : 251 secs\n","output_type":"stream"},{"name":"stderr","text":"\n 59%|█████▉    | 10/17 [00:22<00:15,  2.24s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"name 'promplt_options' is not defined 5\ncode_answers 0 text_answers 0\n\n\n\nQUESTION 0 - 10 - TIME_SPENT : 251 secs\n","output_type":"stream"},{"name":"stderr","text":"\n 65%|██████▍   | 11/17 [00:24<00:13,  2.24s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"name 'promplt_options' is not defined 5\ncode_answers 0 text_answers 0\n\n\n\nQUESTION 0 - 11 - TIME_SPENT : 251 secs\n","output_type":"stream"},{"name":"stderr","text":"\n 71%|███████   | 12/17 [00:26<00:11,  2.25s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"name 'promplt_options' is not defined 5\ncode_answers 0 text_answers 0\n\n\n\nQUESTION 0 - 12 - TIME_SPENT : 251 secs\n","output_type":"stream"},{"name":"stderr","text":"\n 76%|███████▋  | 13/17 [00:29<00:08,  2.24s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"name 'promplt_options' is not defined 5\ncode_answers 0 text_answers 0\n\n\n\nQUESTION 0 - 13 - TIME_SPENT : 251 secs\n","output_type":"stream"},{"name":"stderr","text":"\n 82%|████████▏ | 14/17 [00:31<00:06,  2.24s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"name 'promplt_options' is not defined 5\ncode_answers 0 text_answers 0\n\n\n\nQUESTION 0 - 14 - TIME_SPENT : 251 secs\n","output_type":"stream"},{"name":"stderr","text":"\n 88%|████████▊ | 15/17 [00:33<00:04,  2.24s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"name 'promplt_options' is not defined 5\ncode_answers 0 text_answers 0\n\n\n\nQUESTION 0 - 15 - TIME_SPENT : 251 secs\n","output_type":"stream"},{"name":"stderr","text":"\n 94%|█████████▍| 16/17 [00:35<00:02,  2.24s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"name 'promplt_options' is not defined 5\ncode_answers 0 text_answers 0\n\n\n\nQUESTION 0 - 16 - TIME_SPENT : 251 secs\n","output_type":"stream"},{"name":"stderr","text":"\n100%|██████████| 17/17 [00:38<00:00,  2.24s/it]\u001b[A\n1it [00:38, 38.05s/it]","output_type":"stream"},{"name":"stdout","text":"name 'promplt_options' is not defined 5\ncode_answers 0 text_answers 0\nPredicted best answer: {0: (-1, -1)}\nSolving problem 1 ...\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/17 [00:00<?, ?it/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"\n\n\nQUESTION 1 - 0 - TIME_SPENT : 289 secs\n","output_type":"stream"},{"name":"stderr","text":"\n  6%|▌         | 1/17 [00:02<00:35,  2.23s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"name 'promplt_options' is not defined 5\ncode_answers 0 text_answers 0\n\n\n\nQUESTION 1 - 1 - TIME_SPENT : 289 secs\n","output_type":"stream"},{"name":"stderr","text":"\n 12%|█▏        | 2/17 [00:04<00:33,  2.24s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"name 'promplt_options' is not defined 5\ncode_answers 0 text_answers 0\n\n\n\nQUESTION 1 - 2 - TIME_SPENT : 289 secs\n","output_type":"stream"},{"name":"stderr","text":"\n 18%|█▊        | 3/17 [00:06<00:31,  2.23s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"name 'promplt_options' is not defined 5\ncode_answers 0 text_answers 0\n\n\n\nQUESTION 1 - 3 - TIME_SPENT : 289 secs\n","output_type":"stream"},{"name":"stderr","text":"\n 24%|██▎       | 4/17 [00:08<00:29,  2.23s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"name 'promplt_options' is not defined 5\ncode_answers 0 text_answers 0\n\n\n\nQUESTION 1 - 4 - TIME_SPENT : 289 secs\n","output_type":"stream"},{"name":"stderr","text":"\n 29%|██▉       | 5/17 [00:11<00:26,  2.24s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"name 'promplt_options' is not defined 5\ncode_answers 0 text_answers 0\n\n\n\nQUESTION 1 - 5 - TIME_SPENT : 289 secs\n","output_type":"stream"},{"name":"stderr","text":"\n 35%|███▌      | 6/17 [00:13<00:24,  2.24s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"name 'promplt_options' is not defined 5\ncode_answers 0 text_answers 0\n\n\n\nQUESTION 1 - 6 - TIME_SPENT : 289 secs\n","output_type":"stream"},{"name":"stderr","text":"\n 41%|████      | 7/17 [00:15<00:22,  2.24s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"name 'promplt_options' is not defined 5\ncode_answers 0 text_answers 0\n\n\n\nQUESTION 1 - 7 - TIME_SPENT : 289 secs\n","output_type":"stream"},{"name":"stderr","text":"\n 47%|████▋     | 8/17 [00:17<00:20,  2.23s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"name 'promplt_options' is not defined 5\ncode_answers 0 text_answers 0\n\n\n\nQUESTION 1 - 8 - TIME_SPENT : 289 secs\n","output_type":"stream"},{"name":"stderr","text":"\n 53%|█████▎    | 9/17 [00:20<00:17,  2.25s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"name 'promplt_options' is not defined 5\ncode_answers 0 text_answers 0\n\n\n\nQUESTION 1 - 9 - TIME_SPENT : 289 secs\n","output_type":"stream"},{"name":"stderr","text":"\n 59%|█████▉    | 10/17 [00:22<00:15,  2.25s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"name 'promplt_options' is not defined 5\ncode_answers 0 text_answers 0\n\n\n\nQUESTION 1 - 10 - TIME_SPENT : 289 secs\n","output_type":"stream"},{"name":"stderr","text":"\n 65%|██████▍   | 11/17 [00:24<00:13,  2.24s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"name 'promplt_options' is not defined 5\ncode_answers 0 text_answers 0\n\n\n\nQUESTION 1 - 11 - TIME_SPENT : 289 secs\n","output_type":"stream"},{"name":"stderr","text":"\n 71%|███████   | 12/17 [00:26<00:11,  2.25s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"name 'promplt_options' is not defined 5\ncode_answers 0 text_answers 0\n\n\n\nQUESTION 1 - 12 - TIME_SPENT : 289 secs\n","output_type":"stream"},{"name":"stderr","text":"\n 76%|███████▋  | 13/17 [00:29<00:08,  2.24s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"name 'promplt_options' is not defined 5\ncode_answers 0 text_answers 0\n\n\n\nQUESTION 1 - 13 - TIME_SPENT : 289 secs\n","output_type":"stream"},{"name":"stderr","text":"\n 82%|████████▏ | 14/17 [00:31<00:06,  2.24s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"name 'promplt_options' is not defined 5\ncode_answers 0 text_answers 0\n\n\n\nQUESTION 1 - 14 - TIME_SPENT : 289 secs\n","output_type":"stream"},{"name":"stderr","text":"\n 88%|████████▊ | 15/17 [00:33<00:04,  2.24s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"name 'promplt_options' is not defined 5\ncode_answers 0 text_answers 0\n\n\n\nQUESTION 1 - 15 - TIME_SPENT : 289 secs\n","output_type":"stream"},{"name":"stderr","text":"\n 94%|█████████▍| 16/17 [00:35<00:02,  2.24s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"name 'promplt_options' is not defined 5\ncode_answers 0 text_answers 0\n\n\n\nQUESTION 1 - 16 - TIME_SPENT : 289 secs\n","output_type":"stream"},{"name":"stderr","text":"\n100%|██████████| 17/17 [00:38<00:00,  2.24s/it]\u001b[A\n2it [01:16, 38.04s/it]","output_type":"stream"},{"name":"stdout","text":"name 'promplt_options' is not defined 5\ncode_answers 0 text_answers 0\nPredicted best answer: {0: (-1, -1), 1: (-1, -1)}\nSolving problem 2 ...\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/17 [00:00<?, ?it/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"\n\n\nQUESTION 2 - 0 - TIME_SPENT : 328 secs\n","output_type":"stream"},{"name":"stderr","text":"\n  6%|▌         | 1/17 [00:02<00:35,  2.21s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"name 'promplt_options' is not defined 5\ncode_answers 0 text_answers 0\n\n\n\nQUESTION 2 - 1 - TIME_SPENT : 328 secs\n","output_type":"stream"},{"name":"stderr","text":"\n 12%|█▏        | 2/17 [00:04<00:33,  2.25s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"name 'promplt_options' is not defined 5\ncode_answers 0 text_answers 0\n\n\n\nQUESTION 2 - 2 - TIME_SPENT : 328 secs\n","output_type":"stream"},{"name":"stderr","text":"\n 18%|█▊        | 3/17 [00:06<00:31,  2.25s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"name 'promplt_options' is not defined 5\ncode_answers 0 text_answers 0\n\n\n\nQUESTION 2 - 3 - TIME_SPENT : 328 secs\n","output_type":"stream"},{"name":"stderr","text":"\n 24%|██▎       | 4/17 [00:08<00:29,  2.24s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"name 'promplt_options' is not defined 5\ncode_answers 0 text_answers 0\n\n\n\nQUESTION 2 - 4 - TIME_SPENT : 328 secs\n","output_type":"stream"},{"name":"stderr","text":"\n 29%|██▉       | 5/17 [00:11<00:26,  2.23s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"name 'promplt_options' is not defined 5\ncode_answers 0 text_answers 0\n\n\n\nQUESTION 2 - 5 - TIME_SPENT : 328 secs\n","output_type":"stream"},{"name":"stderr","text":"\n 35%|███▌      | 6/17 [00:13<00:24,  2.24s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"name 'promplt_options' is not defined 5\ncode_answers 0 text_answers 0\n\n\n\nQUESTION 2 - 6 - TIME_SPENT : 328 secs\n","output_type":"stream"},{"name":"stderr","text":"\n 41%|████      | 7/17 [00:15<00:22,  2.24s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"name 'promplt_options' is not defined 5\ncode_answers 0 text_answers 0\n\n\n\nQUESTION 2 - 7 - TIME_SPENT : 328 secs\n","output_type":"stream"},{"name":"stderr","text":"\n 47%|████▋     | 8/17 [00:17<00:20,  2.24s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"name 'promplt_options' is not defined 5\ncode_answers 0 text_answers 0\n\n\n\nQUESTION 2 - 8 - TIME_SPENT : 328 secs\n","output_type":"stream"},{"name":"stderr","text":"\n 53%|█████▎    | 9/17 [00:20<00:17,  2.23s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"name 'promplt_options' is not defined 5\ncode_answers 0 text_answers 0\n\n\n\nQUESTION 2 - 9 - TIME_SPENT : 328 secs\n","output_type":"stream"},{"name":"stderr","text":"\n 59%|█████▉    | 10/17 [00:22<00:15,  2.23s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"name 'promplt_options' is not defined 5\ncode_answers 0 text_answers 0\n\n\n\nQUESTION 2 - 10 - TIME_SPENT : 328 secs\n","output_type":"stream"},{"name":"stderr","text":"\n 65%|██████▍   | 11/17 [00:24<00:13,  2.23s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"name 'promplt_options' is not defined 5\ncode_answers 0 text_answers 0\n\n\n\nQUESTION 2 - 11 - TIME_SPENT : 328 secs\n","output_type":"stream"},{"name":"stderr","text":"\n 71%|███████   | 12/17 [00:26<00:11,  2.24s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"name 'promplt_options' is not defined 5\ncode_answers 0 text_answers 0\n\n\n\nQUESTION 2 - 12 - TIME_SPENT : 328 secs\n","output_type":"stream"},{"name":"stderr","text":"\n 76%|███████▋  | 13/17 [00:29<00:08,  2.24s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"name 'promplt_options' is not defined 5\ncode_answers 0 text_answers 0\n\n\n\nQUESTION 2 - 13 - TIME_SPENT : 328 secs\n","output_type":"stream"},{"name":"stderr","text":"\n 82%|████████▏ | 14/17 [00:31<00:06,  2.23s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"name 'promplt_options' is not defined 5\ncode_answers 0 text_answers 0\n\n\n\nQUESTION 2 - 14 - TIME_SPENT : 328 secs\n","output_type":"stream"},{"name":"stderr","text":"\n 88%|████████▊ | 15/17 [00:33<00:04,  2.23s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"name 'promplt_options' is not defined 5\ncode_answers 0 text_answers 0\n\n\n\nQUESTION 2 - 15 - TIME_SPENT : 328 secs\n","output_type":"stream"},{"name":"stderr","text":"\n 94%|█████████▍| 16/17 [00:35<00:02,  2.23s/it]\u001b[A","output_type":"stream"},{"name":"stdout","text":"name 'promplt_options' is not defined 5\ncode_answers 0 text_answers 0\n\n\n\nQUESTION 2 - 16 - TIME_SPENT : 328 secs\n","output_type":"stream"},{"name":"stderr","text":"\n100%|██████████| 17/17 [00:38<00:00,  2.24s/it]\u001b[A\n3it [01:54, 38.04s/it]","output_type":"stream"},{"name":"stdout","text":"name 'promplt_options' is not defined 5\ncode_answers 0 text_answers 0\nPredicted best answer: {0: (-1, -1), 1: (-1, -1), 2: (-1, -1)}\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"# import numpy as np\n# if PRIVATE:\n#     for ii in range(len(df)):\n#         a = total_answers[ii]\n#         b = total_answers[ii]\n#         a = np.array(a)\n#         b = np.array(b)\n#         print(a,b)\n#         a[a < 0] = b[a < 0]\n\n#         pred = Counter(a.tolist()).most_common(2)\n#         print(pred)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T07:00:32.660394Z","iopub.execute_input":"2024-05-04T07:00:32.660666Z","iopub.status.idle":"2024-05-04T07:00:32.664955Z","shell.execute_reply.started":"2024-05-04T07:00:32.660641Z","shell.execute_reply":"2024-05-04T07:00:32.663929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if PRIVATE:\n#     df['answer'] = [best_stats[ii][0] for ii in range(len(df))]\n# else:\n#     df['answer'] = 2","metadata":{"execution":{"iopub.status.busy":"2024-05-04T07:00:32.66628Z","iopub.execute_input":"2024-05-04T07:00:32.666603Z","iopub.status.idle":"2024-05-04T07:00:32.680081Z","shell.execute_reply.started":"2024-05-04T07:00:32.666572Z","shell.execute_reply":"2024-05-04T07:00:32.6793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df[['id','answer']].to_csv(\"foo.csv\", header=True, index=False)","metadata":{"papermill":{"duration":0.021128,"end_time":"2024-02-29T09:37:05.574782","exception":false,"start_time":"2024-02-29T09:37:05.553654","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-04T07:00:32.681425Z","iopub.execute_input":"2024-05-04T07:00:32.681659Z","iopub.status.idle":"2024-05-04T07:00:32.690243Z","shell.execute_reply.started":"2024-05-04T07:00:32.681638Z","shell.execute_reply":"2024-05-04T07:00:32.689341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if not PRIVATE:\n#     df = pd.read_csv('/kaggle/input/ai-mathematical-olympiad-prize/train.csv')\n#     if PRIVATE:\n#         df['model_answer'] = [best_stats[ii][0] for ii in range(len(df))]\n#         df['match'] = df.answer == df.model_answer\n#         print(f'{df.match.sum()} matches in {len(df)} examples')","metadata":{"execution":{"iopub.status.busy":"2024-05-04T07:00:32.6913Z","iopub.execute_input":"2024-05-04T07:00:32.692931Z","iopub.status.idle":"2024-05-04T07:00:32.701164Z","shell.execute_reply.started":"2024-05-04T07:00:32.692898Z","shell.execute_reply":"2024-05-04T07:00:32.700328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('code.py', 'w') as fout:\n    fout.write(\"print('done')\")\n\nbatcmd = 'timeout 7 ' + sys.executable + ' code.py'\ntry:\n    shell_output = subprocess.check_output(batcmd, shell=True).decode('utf8')\n    print(shell_output)\nexcept:\n    pass","metadata":{"execution":{"iopub.status.busy":"2024-06-08T20:40:11.808232Z","iopub.execute_input":"2024-06-08T20:40:11.808875Z","iopub.status.idle":"2024-06-08T20:40:11.920199Z","shell.execute_reply.started":"2024-06-08T20:40:11.808842Z","shell.execute_reply":"2024-06-08T20:40:11.919283Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"done\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}