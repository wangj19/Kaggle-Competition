{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":66631,"databundleVersionId":8346466,"sourceType":"competition"},{"sourceId":8330401,"sourceType":"datasetVersion","datasetId":4946449},{"sourceId":8449074,"sourceType":"datasetVersion","datasetId":5034873},{"sourceId":148861315,"sourceType":"kernelVersion"},{"sourceId":33551,"sourceType":"modelInstanceVersion","modelInstanceId":28083}],"dockerImageVersionId":30699,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/\n!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/\n!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/\n!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/","metadata":{"execution":{"iopub.status.busy":"2024-05-30T06:57:34.818704Z","iopub.execute_input":"2024-05-30T06:57:34.819548Z","iopub.status.idle":"2024-05-30T06:58:29.941218Z","shell.execute_reply.started":"2024-05-30T06:57:34.819497Z","shell.execute_reply":"2024-05-30T06:58:29.939869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The work in this notebook is inspired by these notebooks:\n* https://www.kaggle.com/code/ivanvybornov/llama3-8b-lgbm-tfidf\n* https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b","metadata":{}},{"cell_type":"markdown","source":"## Importing Libraries","metadata":{}},{"cell_type":"code","source":"import torch\nimport sklearn\nimport numpy as np\nimport pandas as pd\nimport time\n\nfrom transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification, BitsAndBytesConfig\nfrom peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\nfrom torch.cuda.amp import autocast\nfrom threading import Thread\n\nimport gc\nimport os\nimport io\nimport time\nimport json\nimport random\nimport pickle\nimport zipfile\nimport datetime\nimport matplotlib.pyplot as plt\nfrom IPython.display import display\nfrom collections import Counter\nfrom collections import defaultdict\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport pytorch_lightning as pl\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import log_loss\nimport tokenizers\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)\n\nif (not torch.cuda.is_available()): print(\"Sorry - GPU required!\")\n\nMODEL_NAME = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'\nWEIGHTS_PATH = '/kaggle/input/lmsys-model/model'\nMAX_LENGTH = 1284\nBATCH_SIZE = 8\nDEVICE = torch.device(\"cuda\")    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare Data ","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\nsample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# concatenate strings in list\ndef process(input_str):\n    stripped_str = input_str.strip('[]')\n    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n    return  ' '.join(sentences)\n\ntest.loc[:, 'prompt'] = test['prompt'].apply(process)\ntest.loc[:, 'response_a'] = test['response_a'].apply(process)\ntest.loc[:, 'response_b'] = test['response_b'].apply(process)\n\ndisplay(sample_sub)\ndisplay(test.head(5))\n\n# Prepare text for model\ntest['text'] = 'User prompt: ' + test['prompt'] +  '\\n\\nModel A :\\n' + test['response_a'] +'\\n\\n--------\\n\\nModel B:\\n'  + test['response_b']\nprint(test['text'][0])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tokenize","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')\n\ntokens = tokenizer(test['text'].tolist(), padding='max_length',\n                   max_length=MAX_LENGTH, truncation=True, return_tensors='pt')\n\nINPUT_IDS = tokens['input_ids'].to(DEVICE, dtype=torch.int32)\nATTENTION_MASKS = tokens['attention_mask'].to(DEVICE, dtype=torch.int32)\n\n# Move tensors to CPU and convert them to lists\ninput_ids_cpu = [tensor.cpu().tolist() for tensor in INPUT_IDS]\nattention_masks_cpu = [tensor.cpu().tolist() for tensor in ATTENTION_MASKS]\n\ndata = pd.DataFrame()\ndata['INPUT_IDS'] = input_ids_cpu\ndata['ATTENTION_MASKS'] = attention_masks_cpu\ndata[:2]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load model \n> We load 1 model on each gpu.  ","metadata":{}},{"cell_type":"code","source":"# BitsAndBytes configuration\nbnb_config =  BitsAndBytesConfig(\n    load_in_8bit=True,\n    bnb_8bit_compute_dtype=torch.float16,\n    bnb_8bit_use_double_quant=False)\n\n# Load base model on GPU 0\ndevice0 = torch.device('cuda:0')\n\nbase_model_0 = LlamaForSequenceClassification.from_pretrained(\n    MODEL_NAME,\n    num_labels=3,\n    torch_dtype=torch.float16,\n    quantization_config=bnb_config,\n    device_map='cuda:0')\nbase_model_0.config.pad_token_id = tokenizer.pad_token_id","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load base model on GPU 1\ndevice1 = torch.device('cuda:1')\nbase_model_1 = LlamaForSequenceClassification.from_pretrained(\n    MODEL_NAME,\n    num_labels=3,\n    torch_dtype=torch.float16,\n    quantization_config=bnb_config,\n    device_map='cuda:1')\nbase_model_1.config.pad_token_id = tokenizer.pad_token_id","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load weights \n","metadata":{}},{"cell_type":"code","source":"# LoRa configuration\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.10,\n    bias='none',\n    inference_mode=True,\n    task_type=TaskType.SEQ_CLS,\n    target_modules=['o_proj', 'v_proj'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get peft\nmodel_0 = get_peft_model(base_model_0, peft_config).to(device0) \n#Load weights\nmodel_0.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)\nmodel_0.eval()\n\nmodel_1 = get_peft_model(base_model_1, peft_config).to(device1)\nmodel_1.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)\nmodel_1.eval()\n\n#Trainable Parameters\nmodel_0.print_trainable_parameters(), model_1.print_trainable_parameters()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"def inference(df, model, device, batch_size=BATCH_SIZE):\n    input_ids = torch.tensor(df['INPUT_IDS'].values.tolist(), dtype=torch.long)\n    attention_mask = torch.tensor(df['ATTENTION_MASKS'].values.tolist(), dtype=torch.long)\n    \n    generated_class_a = []\n    generated_class_b = []\n    generated_class_c = []\n\n    model.eval()\n    \n    for start_idx in range(0, len(df), batch_size):\n        end_idx = min(start_idx + batch_size, len(df))\n        batch_input_ids = input_ids[start_idx:end_idx].to(device)\n        batch_attention_mask = attention_mask[start_idx:end_idx].to(device)\n        \n        with torch.no_grad():\n            with autocast():\n                outputs = model(\n                    input_ids=batch_input_ids,\n                    attention_mask=batch_attention_mask\n                )\n        \n        probabilities = torch.softmax(outputs.logits, dim=-1).cpu().numpy()\n        \n        generated_class_a.extend(probabilities[:, 0])\n        generated_class_b.extend(probabilities[:, 1])\n        generated_class_c.extend(probabilities[:, 2])\n    \n    df['winner_model_a'] = generated_class_a\n    df['winner_model_b'] = generated_class_b\n    df['winner_tie'] = generated_class_c\n\n    torch.cuda.empty_cache()  \n\n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"st = time.time()\n\nN_SAMPLES = len(data)\n\n# Split the data into two subsets\nhalf = round(N_SAMPLES / 2)\nsub1 = data.iloc[0:half].copy()\nsub2 = data.iloc[half:N_SAMPLES].copy()\n\n# Function to run inference in a thread\ndef run_inference(df, model, device, results, index):\n    results[index] = inference(df, model, device)\n\n# Dictionary to store results from threads\nresults = {}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# start threads\nt0 = Thread(target=run_inference, args=(sub1, model_0, device0, results, 0))\nt1 = Thread(target=run_inference, args=(sub2, model_1, device1, results, 1))\n\nt0.start()\nt1.start()\n\n# Wait for all threads to finish\nt0.join()\nt1.join()\n\n# Combine results back into the original DataFrame\ndata = pd.concat([results[0], results[1]], axis=0)\n\nprint(f\"Processing complete. Total time: {time.time() - st}\")\n\nTARGETS = ['winner_model_a', 'winner_model_b', 'winner_tie']\n\nsample_sub[TARGETS] = data[TARGETS]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"llama_preds = data[TARGETS].values","metadata":{"execution":{"iopub.status.busy":"2024-05-30T07:00:50.825683Z","iopub.execute_input":"2024-05-30T07:00:50.825959Z","iopub.status.idle":"2024-05-30T07:00:50.831376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LGBM + tfidf","metadata":{}},{"cell_type":"code","source":"TAG = 'lmsys-chatbot-arena'\n\nimport os\nRUNPOD = os.path.exists('/workspace/')\nKAGGLE = not RUNPOD\nif KAGGLE: print('kaggle')","metadata":{"execution":{"iopub.status.busy":"2024-05-30T07:00:50.832506Z","iopub.execute_input":"2024-05-30T07:00:50.832861Z","iopub.status.idle":"2024-05-30T07:00:50.853311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    import pandas as pd\nexcept:\n    !pip install -q kaggle\n    !pip install -q pandas matplotlib scipy joblib scikit-learn lightgbm \n    !pip install -q protobuf \n    !pip install -q numba","metadata":{"execution":{"iopub.status.busy":"2024-05-30T07:00:50.854159Z","iopub.execute_input":"2024-05-30T07:00:50.854384Z","iopub.status.idle":"2024-05-30T07:00:50.86715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA = '/data/' if RUNPOD else 'data/' \\\n        if not os.path.exists('/kaggle/') \\\n            else '/kaggle/input/{}/'.format(TAG)\n\nimport os\n\nif RUNPOD:\n    if not os.path.exists('~/.kaggle/kaggle.json'):\n        !mkdir -p ~/.kaggle\n        !cp /workspace/kaggle.json ~/.kaggle/kaggle.json\n        !chmod 600 /root/.kaggle/kaggle.json\n\n    if not os.path.exists('/workspace/' + TAG + '.zip'):\n        !kaggle competitions download $TAG -p /workspace/ \n        \n    if not os.path.exists('/data/'):\n        import zipfile\n        zipfile.ZipFile('/workspace/' + TAG + '.zip').extractall('/data/')    ","metadata":{"execution":{"iopub.status.busy":"2024-05-30T07:00:50.86863Z","iopub.execute_input":"2024-05-30T07:00:50.86901Z","iopub.status.idle":"2024-05-30T07:00:50.88384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"INPUT_PATH = '/kaggle/input/'  \nMODEL_PATH = '/workspace/models/'; LOGITS_PATH = '/workspace/logits/'\nMODEL_PATH = MODEL_PATH if not KAGGLE else '/kaggle/input/' \\\n                + [e for e in os.listdir('/kaggle/input') if 'lsys-models' in e][0] + '/'\n# MODEL_PATH = MODEL_PATH if not KAGGLE else ''#MODEL_PATH + os.listdir(MODEL_PATH)[0] + '/'\nprint(MODEL_PATH)\n\nCODE_PATH = MODEL_PATH if KAGGLE else '/workspace/'\nSAVE_PATH = MODEL_PATH if not KAGGLE else ''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ['TOKENIZERS_PARALLELISM'] = 'false'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(open(DATA + 'train.csv', 'r'))\ntest = pd.read_csv(open(DATA + 'test.csv', 'r'))\nsample = pd.read_csv(DATA + 'sample_submission.csv')\n\nprint(len(train), len(test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {}\nif False:#len(test) < 10: \n    pass;\n    params['subsample'] = 30\nelse:\n    # params['subsample'] = 2\n    params['fold'] = -1\n\n\nparams['n_epochs'] = 1\nparams['n_lgb'] = 1\nparams['model'] = 'microsoft/deberta-v3-small'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# params = {}\nFULL = params.get('fold', 0) < 0\nN_FOLDS = int(params.get('n_folds', 3)); \nFOLD = int(params.get('fold', 0))\nSEED = int(params.get('seed', 3))\nSS = int(params.get('subsample', 1))\n\nprint(N_FOLDS, FOLD, SEED, SS)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\n\ndef get_folds(train): \n    return list(StratifiedKFold(N_FOLDS, random_state = SEED, shuffle = True)\\\n                    .split(X = np.zeros(len(train)), y = train.iloc[:, -3:].idxmax(1)))\n\ntrain_ids, test_ids = get_folds(train)[FOLD] if not FULL else [list(range(len(train))), []]\nif SS > 1: train_ids, test_ids = train_ids[::SS], test_ids[::SS]\n\nprint(len(train_ids), len(test_ids));  assert set(train_ids) & set(test_ids) == set() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def join_strings(x, ):\n    x = ' '.join(['' if e is None else e for e in x]) if isinstance(x, list) else x\n    return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def len_join_strings(x, ):\n    return len(join_strings(x).split())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def len_join_strings_j(x):\n    x = json.loads(x)\n    return len_join_strings(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(datetime.datetime.now().microsecond)\nrandom.seed(datetime.datetime.now().microsecond)\nnp.random.seed(datetime.datetime.now().microsecond)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TRAIN = True and not KAGGLE\nTRAIN = False\nINFER = True # or KAGGLE \nSAVE = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.feature_extraction.text import CountVectorizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LGB = True\nTRAIN_LGB = TRAIN and LGB and params.get('n_lgb', 1) > 0\nINFER_LGB = not TRAIN and LGB","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cvec  = pickle.load(open(MODEL_PATH + 'cvec.pkl', 'rb'))\nccvec = pickle.load(open(MODEL_PATH + 'ccvec.pkl', 'rb'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def symlog(x): return (np.sign(x) * np.log1p(np.abs(x))).astype(np.float32)\n\ndef dense(x):\n    x = np.asarray(x.astype(np.float32).todense())\n    x = symlog(x)\n    return x\n\ndef get_features(df):\n    pfeat = np.hstack([dense(v.transform(df[c])) \n                for v in [cvec, ccvec]\n                    for c in ['prompt', ]])\n    afeat = np.hstack([dense(v.transform(df[c])) \n                for c in ['response_a', ]\n                    for v in [cvec, ccvec]\n                ])\n    bfeat = np.hstack([dense(v.transform(df[c])) \n                for c in ['response_b', ]\n                    for v in [cvec, ccvec]\n                ])\n    \n    v = np.hstack([\n    # pfeat, \n          afeat - bfeat, np.abs(afeat - bfeat), \n    # afeat + bfeat\n        ])\n    try: \n        v = v / (len(all_vote_models) if len(df) < len(train) else 1)\n    except: pass\n\n    extras = []\n    EXTRAS = ['\\n', '\\n\\n', '.', ' ', '\",\"']\n    for e in EXTRAS:\n        for c in ['prompt', 'response_a', 'response_b']:\n            extras.append(df[c].str.count(e).values)\n            \n    extras.append(df[c].str.len())\n    extras.append(df[c].str.split().apply(lambda x: len(x)))\n    \n    extras = np.stack(extras, axis = 1)\n    extras = np.hstack([extras ** 0.5, np.log1p(extras)])\n    return np.hstack([v, extras])\n    # return v\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgb_models = pickle.load(open(MODEL_PATH + 'lgb_models.pkl', 'rb'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if INFER and params.get('n_lgb', 1) > 0:\n    df = test\n    yps = []; b = 1000\n    for i in range(0, len(df), b):\n        arr = get_features(df.iloc[i: i + b])\n        ypms = []\n        for model in lgb_models:\n            ypms.append(model.predict_proba(arr))\n        yps.append(np.stack(ypms).mean(0))\n        # break;\n        print('.', end = '')\n        \n        if len(yps) % 2 == 0:\n            gc.collect()\n    print()\n\n    yp = np.concatenate(yps)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgb_preds = yp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Blend predictions\n\n$\\operatorname{preds} = 0.2 \\cdot \\operatorname{lgbm boosting preds} + 0.8 \\cdot \\operatorname{llama preds}$\n","metadata":{}},{"cell_type":"code","source":"lgb_wt = 0.2 \npreds = lgb_wt * lgb_preds + (1 - lgb_wt) * llama_preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out = pd.DataFrame(preds, \n                index = df.id, \n                    columns = train.columns[-3:])\ndisplay(out.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out.to_csv('submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}