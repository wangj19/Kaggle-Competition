{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":66631,"databundleVersionId":8346466,"sourceType":"competition"},{"sourceId":6703755,"sourceType":"datasetVersion","datasetId":3863727},{"sourceId":33551,"sourceType":"modelInstanceVersion","modelInstanceId":28083}],"dockerImageVersionId":30734,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#  Llama-3 8b [TPU Train]\n\nLearning to train llms on tpu, Hope this will help you too!\n\nNotebook inspired from:\n\n* [LLM detect AI comp Mistral-7B](https://www.kaggle.com/code/hotchpotch/train-llm-detect-ai-comp-mistral-7b/notebook)\n* [DAIGT Mistral-7B TPU BFloat16 [Train]](https://www.kaggle.com/code/markwijkhuizen/daigt-mistral-7b-tpu-bfloat16-train)\n* [LLAMA 2 13B on TPU (Training)](https://www.kaggle.com/code/defdet/llama-2-13b-on-tpu-training)\n\n\nPrerequisite: Access to using llama-3\n\nNote: This is only training notebook, you can find inference notebook [here](https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b)\n\nPlease upvote if you learn or find this helpful!","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# Import libs ","metadata":{}},{"cell_type":"code","source":"# Install libs\n!pip install -qq peft==0.6.0\n!pip install -qq bitsandbytes==0.41.1\n!pip install -qq accelerate==0.24.1\n!pip install -qq transformers==4.35.0\n!pip install -qq torch~=2.1.0 --index-url https://download.pytorch.org/whl/cpu -q \n!pip install -qq torch_xla[tpu]~=2.1.0 -f https://storage.googleapis.com/libtpu-releases/index.html -q\n!pip uninstall -qq tensorflow -y # If we don't do this, TF will take over TPU and cause permission error for PT\n!cp /kaggle/input/utils-xla/spmd_util.py . # From this repo: https://github.com/HeegyuKim/torch-xla-SPMD","metadata":{"execution":{"iopub.status.busy":"2024-06-14T14:43:34.908481Z","iopub.execute_input":"2024-06-14T14:43:34.9091Z","iopub.status.idle":"2024-06-14T14:45:33.530005Z","shell.execute_reply.started":"2024-06-14T14:43:34.909071Z","shell.execute_reply":"2024-06-14T14:45:33.528883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport re\nfrom time import time\nimport random\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\n\nimport torch\nimport transformers\nfrom sklearn.metrics import accuracy_score\nfrom transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification\nfrom peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\nimport torch.nn.functional as F\n\nimport torch_xla.debug.profiler as xp\nimport torch_xla.core.xla_model as xm\nimport torch_xla.experimental.xla_sharding as xs\nimport torch_xla.runtime as xr\n\nxr.use_spmd()\n\nfrom torch_xla.experimental.xla_sharded_tensor import XLAShardedTensor\nfrom torch_xla.experimental.xla_sharding import Mesh\nfrom spmd_util import partition_module\n\ntqdm.pandas()\n\nprint(f'Torch Version: {torch.__version__}')","metadata":{"execution":{"iopub.status.busy":"2024-06-14T14:45:33.531832Z","iopub.execute_input":"2024-06-14T14:45:33.532115Z","iopub.status.idle":"2024-06-14T14:45:50.280982Z","shell.execute_reply.started":"2024-06-14T14:45:33.532084Z","shell.execute_reply":"2024-06-14T14:45:50.280254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Configs","metadata":{}},{"cell_type":"code","source":"class CFG:\n    NUM_EPOCHS = 1\n    BATCH_SIZE = 32\n    DROPOUT = 0.05 \n    MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'\n    SEED = 2024 \n    MAX_LENGTH = 1024 \n    NUM_WARMUP_STEPS = 128\n    LR_MAX = 5e-5 \n    NUM_LABELS = 3 \n    LORA_RANK = 4\n    LORA_ALPHA = 8\n    LORA_MODULES = ['o_proj', 'v_proj']\n    \nDEVICE = xm.xla_device() # Initialize TPU Device","metadata":{"execution":{"iopub.status.busy":"2024-06-14T14:47:11.281776Z","iopub.execute_input":"2024-06-14T14:47:11.282545Z","iopub.status.idle":"2024-06-14T14:47:11.287356Z","shell.execute_reply.started":"2024-06-14T14:47:11.282501Z","shell.execute_reply":"2024-06-14T14:47:11.286652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seeds(seed):\n    \"\"\"Set seeds for reproducibility \"\"\"\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        \n    # Set seed for all TPU cores\n    xm.set_rng_state(seed, device=xm.xla_device())  \n\nset_seeds(seed=CFG.SEED)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T14:47:14.759611Z","iopub.execute_input":"2024-06-14T14:47:14.759949Z","iopub.status.idle":"2024-06-14T14:47:14.76582Z","shell.execute_reply.started":"2024-06-14T14:47:14.759918Z","shell.execute_reply":"2024-06-14T14:47:14.765152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenizer","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(CFG.MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = 'right'\ntokenizer.add_eos_token = True\n\n# save tokenizer to load offline during inference\ntokenizer.save_pretrained('tokenizer')","metadata":{"execution":{"iopub.status.busy":"2024-06-14T14:47:15.592637Z","iopub.execute_input":"2024-06-14T14:47:15.59299Z","iopub.status.idle":"2024-06-14T14:47:16.251115Z","shell.execute_reply.started":"2024-06-14T14:47:15.592957Z","shell.execute_reply":"2024-06-14T14:47:16.250461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Utility function giving token length\ndef get_token_lengths(texts):\n    # tokenize and receive input_ids for reach text\n    input_ids = tokenizer(texts.tolist(), return_tensors='np')['input_ids']\n    # return length of inputs_ids for each text\n    return [len(t) for t in input_ids]","metadata":{"execution":{"iopub.status.busy":"2024-06-14T14:47:16.252452Z","iopub.execute_input":"2024-06-14T14:47:16.252742Z","iopub.status.idle":"2024-06-14T14:47:16.256526Z","shell.execute_reply.started":"2024-06-14T14:47:16.252713Z","shell.execute_reply":"2024-06-14T14:47:16.255914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare train\n","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')\ndef process(input_str):\n    stripped_str = input_str.strip('[]')\n    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n    return  ' '.join(sentences)\n\ntrain.loc[:, 'prompt'] = train['prompt'].apply(process)\ntrain.loc[:, 'response_a'] = train['response_a'].apply(process)\ntrain.loc[:, 'response_b'] = train['response_b'].apply(process)\n\n# Drop 'Null' for training\nindexes = train[(train.response_a == 'null') & (train.response_b == 'null')].index\ntrain.drop(indexes, inplace=True)\ntrain.reset_index(inplace=True, drop=True)\n\nprint(f\"Total {len(indexes)} Null response rows dropped\")\nprint('Total train samples: ', len(train))","metadata":{"execution":{"iopub.status.busy":"2024-06-14T14:47:17.472119Z","iopub.execute_input":"2024-06-14T14:47:17.472488Z","iopub.status.idle":"2024-06-14T14:47:20.907187Z","shell.execute_reply.started":"2024-06-14T14:47:17.472456Z","shell.execute_reply":"2024-06-14T14:47:20.906288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T14:47:20.90852Z","iopub.execute_input":"2024-06-14T14:47:20.908785Z","iopub.status.idle":"2024-06-14T14:47:20.921893Z","shell.execute_reply.started":"2024-06-14T14:47:20.90876Z","shell.execute_reply":"2024-06-14T14:47:20.921234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['text'] = 'User prompt: ' + train['prompt'] +  '\\n\\nModel A :\\n' + train['response_a'] +'\\n\\n--------\\n\\nModel B:\\n'  + train['response_b']\nprint(train['text'][4])","metadata":{"execution":{"iopub.status.busy":"2024-06-14T14:47:20.922797Z","iopub.execute_input":"2024-06-14T14:47:20.923087Z","iopub.status.idle":"2024-06-14T14:47:20.935328Z","shell.execute_reply.started":"2024-06-14T14:47:20.923057Z","shell.execute_reply":"2024-06-14T14:47:20.934591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Train with only take 50% train dataset\n# train = train[:int(len(train) * 0.5)]\n\ntrain.loc[:, 'token_count'] = get_token_lengths(train['text'])\n\n# prepare label for model\ntrain.loc[:, 'label'] = np.argmax(train[['winner_model_a','winner_model_b','winner_tie']].values, axis=1)\n\n# Display data\ndisplay(train.head())","metadata":{"execution":{"iopub.status.busy":"2024-06-14T14:47:20.93686Z","iopub.execute_input":"2024-06-14T14:47:20.937105Z","iopub.status.idle":"2024-06-14T14:47:21.061297Z","shell.execute_reply.started":"2024-06-14T14:47:20.93708Z","shell.execute_reply":"2024-06-14T14:47:21.060599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.label.value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-06-14T14:47:21.062289Z","iopub.execute_input":"2024-06-14T14:47:21.06261Z","iopub.status.idle":"2024-06-14T14:47:21.068687Z","shell.execute_reply.started":"2024-06-14T14:47:21.062579Z","shell.execute_reply":"2024-06-14T14:47:21.068077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# token Count\ndisplay(train['token_count'].describe().to_frame().astype(int))","metadata":{"execution":{"iopub.status.busy":"2024-06-14T14:47:21.069531Z","iopub.execute_input":"2024-06-14T14:47:21.069845Z","iopub.status.idle":"2024-06-14T14:47:21.087002Z","shell.execute_reply.started":"2024-06-14T14:47:21.069817Z","shell.execute_reply":"2024-06-14T14:47:21.086386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get length of tokens which covers 90% of data, we'll still take 1024 length!\nnp.percentile(train['token_count'], 90)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T14:47:21.087907Z","iopub.execute_input":"2024-06-14T14:47:21.08819Z","iopub.status.idle":"2024-06-14T14:47:21.097766Z","shell.execute_reply.started":"2024-06-14T14:47:21.088163Z","shell.execute_reply":"2024-06-14T14:47:21.097087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenize","metadata":{}},{"cell_type":"code","source":"# Tokenize Data\ntokens = tokenizer(\n    train['text'].tolist(), \n    padding='max_length', \n    max_length=CFG.MAX_LENGTH, \n    truncation=True, \n    return_tensors='np')\n\n# Input IDs are the token IDs\nINPUT_IDS = tokens['input_ids']\n# Attention Masks to Ignore Padding Tokens\nATTENTION_MASKS = tokens['attention_mask']\n# Label of Texts\nLABELS = train[['winner_model_a','winner_model_b','winner_tie']].values\n\nprint(f'INPUT_IDS shape: {INPUT_IDS.shape}, ATTENTION_MASKS shape: {ATTENTION_MASKS.shape}')\nprint(f'LABELS shape: {LABELS.shape}')","metadata":{"execution":{"iopub.status.busy":"2024-06-14T14:47:21.098627Z","iopub.execute_input":"2024-06-14T14:47:21.098869Z","iopub.status.idle":"2024-06-14T14:47:21.164529Z","shell.execute_reply.started":"2024-06-14T14:47:21.098845Z","shell.execute_reply":"2024-06-14T14:47:21.163719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_dataset(batch_size):\n    N_SAMPLES = LABELS.shape[0]\n    IDXS = np.arange(N_SAMPLES - (N_SAMPLES % batch_size))\n    while True:\n        # Shuffle Indices\n        np.random.shuffle(IDXS)\n        # Iterate Over All Indices Once\n        for idxs in IDXS.reshape(-1, batch_size):\n            input_ids = torch.tensor(INPUT_IDS[idxs]).to(DEVICE)\n            attention_mask = torch.tensor(ATTENTION_MASKS[idxs]).to(DEVICE)\n            labels = torch.tensor(LABELS[idxs]).to(DEVICE)  # Multi-label output\n            \n            # Shard Over TPU Nodes if applicable (you need to define mesh appropriately)\n            xs.mark_sharding(input_ids, mesh, (0, 1))\n            xs.mark_sharding(attention_mask, mesh, (0, 1))\n            xs.mark_sharding(labels, mesh, (0, 1))\n            \n            yield input_ids, attention_mask, labels\n\nTRAIN_DATASET = train_dataset(CFG.BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T14:47:21.165514Z","iopub.execute_input":"2024-06-14T14:47:21.165785Z","iopub.status.idle":"2024-06-14T14:47:21.171477Z","shell.execute_reply.started":"2024-06-14T14:47:21.165759Z","shell.execute_reply":"2024-06-14T14:47:21.170724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Model","metadata":{}},{"cell_type":"code","source":"# Load model for classification with 3 target label\nbase_model = LlamaForSequenceClassification.from_pretrained(\n    CFG.MODEL_NAME,\n    num_labels=CFG.NUM_LABELS,\n    torch_dtype=torch.bfloat16)\n\nbase_model.config.pretraining_tp = 1 \n\n# Assign Padding TOKEN\nbase_model.config.pad_token_id = tokenizer.pad_token_id","metadata":{"execution":{"iopub.status.busy":"2024-06-14T14:47:21.173573Z","iopub.execute_input":"2024-06-14T14:47:21.173887Z","iopub.status.idle":"2024-06-14T14:49:12.730676Z","shell.execute_reply.started":"2024-06-14T14:47:21.173851Z","shell.execute_reply":"2024-06-14T14:49:12.729882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Low-Rank Adaptation [LORA]","metadata":{}},{"cell_type":"code","source":"lora_config = LoraConfig(\n    r=CFG.LORA_RANK,  # the dimension of the low-rank matrices\n    lora_alpha = CFG.LORA_ALPHA, # scaling factor for LoRA activations vs pre-trained weight activations\n    lora_dropout= CFG.DROPOUT, \n    bias='none',\n    inference_mode=False,\n    task_type=TaskType.SEQ_CLS,\n    target_modules=CFG.LORA_MODULES ) # Only Use Output and Values Projection","metadata":{"execution":{"iopub.status.busy":"2024-06-14T14:49:12.731635Z","iopub.execute_input":"2024-06-14T14:49:12.731935Z","iopub.status.idle":"2024-06-14T14:49:12.736026Z","shell.execute_reply.started":"2024-06-14T14:49:12.731892Z","shell.execute_reply":"2024-06-14T14:49:12.735294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create LoRa Model\nmodel = get_peft_model(base_model, lora_config)\n# Trainable Parameters\nmodel.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-06-14T14:49:12.736911Z","iopub.execute_input":"2024-06-14T14:49:12.737158Z","iopub.status.idle":"2024-06-14T14:49:12.836482Z","shell.execute_reply.started":"2024-06-14T14:49:12.737133Z","shell.execute_reply":"2024-06-14T14:49:12.835761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of TPU Nodes\nnum_devices = xr.global_runtime_device_count()\nmesh_shape = (1, num_devices, 1)\ndevice_ids = np.array(range(num_devices))\nmesh = Mesh(device_ids, mesh_shape, ('dp', 'fsdp', 'mp'))\n# distribute model\npartition_module(model, mesh)\n\nprint(f'num_devices: {num_devices}')","metadata":{"execution":{"iopub.status.busy":"2024-06-14T14:49:12.838116Z","iopub.execute_input":"2024-06-14T14:49:12.838524Z","iopub.status.idle":"2024-06-14T14:49:34.052077Z","shell.execute_reply.started":"2024-06-14T14:49:12.838497Z","shell.execute_reply":"2024-06-14T14:49:34.051008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Verfy The Trainable Layers\nMODEL_LAYERS_ROWS = []\nTRAINABLE_PARAMS = []\nN_TRAINABLE_PARAMS = 0\n\nfor name, param in model.named_parameters():\n    # Layer Parameter Count\n    n_parameters = int(torch.prod(torch.tensor(param.shape)))\n    # Only Trainable Layers\n    if param.requires_grad:\n        # Add Layer Information\n        MODEL_LAYERS_ROWS.append({\n            'param': n_parameters,\n            'name': name,\n            'dtype': param.data.dtype,\n        })\n        # Append Trainable Parameter\n        TRAINABLE_PARAMS.append({ 'params': param })\n        # Add Number Of Trainable Parameters\"\n        N_TRAINABLE_PARAMS += n_parameters\n        \ndisplay(pd.DataFrame(MODEL_LAYERS_ROWS))\n\nprint(f\"\"\"\n===============================\nN_TRAINABLE_PARAMS: {N_TRAINABLE_PARAMS:,}\nN_TRAINABLE_LAYERS: {len(TRAINABLE_PARAMS)}\n===============================\n\"\"\")","metadata":{"execution":{"iopub.status.busy":"2024-06-14T14:49:34.0531Z","iopub.execute_input":"2024-06-14T14:49:34.053363Z","iopub.status.idle":"2024-06-14T14:49:34.074759Z","shell.execute_reply.started":"2024-06-14T14:49:34.053325Z","shell.execute_reply":"2024-06-14T14:49:34.073683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"# LR & Optimizer\nN_SAMPLES = len(train)\nSTEPS_PER_EPOCH = N_SAMPLES // CFG.BATCH_SIZE\n\nOPTIMIZER = torch.optim.AdamW(model.parameters(), lr=CFG.LR_MAX)\n\n# Cosine Learning Rate With Warmup\nlr_scheduler = transformers.get_cosine_schedule_with_warmup(\n    optimizer=OPTIMIZER,\n    num_warmup_steps=CFG.NUM_WARMUP_STEPS,\n    num_training_steps=STEPS_PER_EPOCH * CFG.NUM_EPOCHS)\n\nprint(f'BATCH_SIZE: {CFG.BATCH_SIZE}, N_SAMPLES: {N_SAMPLES}, STEPS_PER_EPOCH: {STEPS_PER_EPOCH}')","metadata":{"execution":{"iopub.status.busy":"2024-06-14T14:49:34.076025Z","iopub.execute_input":"2024-06-14T14:49:34.076286Z","iopub.status.idle":"2024-06-14T14:49:34.084474Z","shell.execute_reply.started":"2024-06-14T14:49:34.076261Z","shell.execute_reply":"2024-06-14T14:49:34.083536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the data type for the optimizer's state (e.g., momentum buffers)\nfor state in OPTIMIZER.state.values():\n    for k, v in state.items():\n        if isinstance(v, torch.Tensor) and state[k].dtype is not torch.float32:\n            state[v] = v.to(dtype=torch.float32)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T14:49:34.085575Z","iopub.execute_input":"2024-06-14T14:49:34.085881Z","iopub.status.idle":"2024-06-14T14:49:34.099097Z","shell.execute_reply.started":"2024-06-14T14:49:34.085847Z","shell.execute_reply":"2024-06-14T14:49:34.098375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_ids, attention_mask, labels = next(TRAIN_DATASET)\n\nprint(f'input_ids shape: {input_ids.shape}, dtype: {input_ids.dtype}')\nprint(f'attention_mask shape: {attention_mask.shape}, dtype: {attention_mask.dtype}')\nprint(f'labels shape: {labels.shape}, dtype: {labels.dtype}')","metadata":{"execution":{"iopub.status.busy":"2024-06-14T14:49:34.099963Z","iopub.execute_input":"2024-06-14T14:49:34.100193Z","iopub.status.idle":"2024-06-14T14:49:34.113207Z","shell.execute_reply.started":"2024-06-14T14:49:34.10017Z","shell.execute_reply":"2024-06-14T14:49:34.112381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Dummy Prediction\nwith torch.no_grad():\n    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n    \nprint(f'logits: {outputs.logits}, dtype: {outputs.logits.dtype}')","metadata":{"execution":{"iopub.status.busy":"2024-06-14T14:49:34.114328Z","iopub.execute_input":"2024-06-14T14:49:34.114643Z","iopub.status.idle":"2024-06-14T14:50:01.390143Z","shell.execute_reply.started":"2024-06-14T14:49:34.114611Z","shell.execute_reply":"2024-06-14T14:50:01.389094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Put Model In Train Mode\nmodel.train()\n\n# Loss Function, Cross Entropy\nLOSS_FN = torch.nn.CrossEntropyLoss().to(dtype=torch.float32)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T14:50:01.391372Z","iopub.execute_input":"2024-06-14T14:50:01.391664Z","iopub.status.idle":"2024-06-14T14:50:01.400293Z","shell.execute_reply.started":"2024-06-14T14:50:01.391635Z","shell.execute_reply":"2024-06-14T14:50:01.399505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"st = time()\nwarnings.filterwarnings(\"error\")\nMETRICS = {\n    'loss': [],\n    'accuracy': {'y_true': [], 'y_pred': [] }}\n\nfor epoch in tqdm(range(CFG.NUM_EPOCHS)):\n    ste = time()\n    for step in range(STEPS_PER_EPOCH):\n        # Zero Out Gradients\n        OPTIMIZER.zero_grad()\n        \n        # Get Batch\n        input_ids, attention_mask, labels = next(TRAIN_DATASET)\n        \n        # Forward Pass\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n       \n        # Logits Float32\n        logits = outputs.logits.to(dtype=torch.float32)\n        \n        # Backward Pass\n        loss = LOSS_FN(logits, labels.to(dtype=torch.float32))\n        loss.backward()\n        \n        # optimizer step\n        OPTIMIZER.step()\n        xm.mark_step()\n        \n        # Update Learning Rate Scheduler\n        lr_scheduler.step()\n        \n        # Update Metrics And Progress Bar\n        METRICS['loss'].append(float(loss))\n        METRICS['accuracy']['y_true'] += labels.squeeze().tolist()\n        METRICS['accuracy']['y_pred'] += torch.argmax(F.softmax(logits, dim=-1), dim=1).cpu().tolist()\n        \n        if (step + 1) % 200 == 0:  \n            metrics = 'µ_loss: {:.3f}'.format(np.mean(METRICS['loss']))\n            metrics += ', step_loss: {:.3f}'.format(METRICS['loss'][-1])\n            metrics += ', µ_auc: {:.3f}'.format(accuracy_score(torch.argmax(torch.tensor(METRICS['accuracy']['y_true']), axis=-1), \\\n                                                               METRICS['accuracy']['y_pred']))\n            lr = OPTIMIZER.param_groups[0]['lr']\n            print(f'{epoch+1:02}/{CFG.NUM_EPOCHS:02} | {step+1:04}/{STEPS_PER_EPOCH} lr: {lr:.2E}, {metrics}', end='')\n            print(f'\\nSteps per epoch: {step+1} complete | Time elapsed: {time()- st}')\n    \n    print(f'\\nEpoch {epoch+1} Completed | Total time for epoch: {time() - ste} ' )\n\n    # If stopped, and to continue training in future on tpu we save model and optimizer\n    xm.save({k: v.cpu() for k, v in model.named_parameters() if v.requires_grad}, f'model_llama_3_cp_{epoch+1}_v1.pth')\n    xm.save(OPTIMIZER.state_dict(), f'optimizer_llama_3_cp_{epoch+1}_v1.pth')    \n    \n    print(f'Model saved at epoch {epoch+1}| Elapsed time: {time() - st} ')","metadata":{"execution":{"iopub.status.busy":"2024-06-14T14:54:33.375489Z","iopub.execute_input":"2024-06-14T14:54:33.375792Z","iopub.status.idle":"2024-06-14T14:54:35.048124Z","shell.execute_reply.started":"2024-06-14T14:54:33.375764Z","shell.execute_reply":"2024-06-14T14:54:35.046592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15, 6))\nplt.plot(METRICS['loss'])    \nplt.xlabel('Step per epoch')\nplt.ylabel('Loss')\nplt.title('Loss Plot step per epoch')    \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-14T14:54:35.048822Z","iopub.status.idle":"2024-06-14T14:54:35.049129Z","shell.execute_reply.started":"2024-06-14T14:54:35.048981Z","shell.execute_reply":"2024-06-14T14:54:35.048996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save Model\n","metadata":{}},{"cell_type":"code","source":"model = model.cpu()\ntorch.save(dict([(k,v) for k, v in model.named_parameters() if v.requires_grad]), 'llama_3_finetuned_model.pth')","metadata":{"execution":{"iopub.status.busy":"2024-06-14T14:52:40.497608Z","iopub.execute_input":"2024-06-14T14:52:40.497887Z","iopub.status.idle":"2024-06-14T14:54:33.373114Z","shell.execute_reply.started":"2024-06-14T14:52:40.49786Z","shell.execute_reply":"2024-06-14T14:54:33.371833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion \n\nThere is still alot of room to speed up and optimize training! Try out more data, different batch size, lr... All the best!","metadata":{}}]}